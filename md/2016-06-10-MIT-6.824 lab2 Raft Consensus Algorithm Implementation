Raft 将一致性问题分为了三个相对独立的子问题，分别是：
- Leader election：当前 leader 崩溃时，集群中必须选举出一个新的 leader；
- Log replication：leader 必须接受来自 clients 的 log entries，并且将其 replicate 到集群机器中，强制其余 logs 与其保持一致；
- Safety：Raft 中最关键的 safety property 是 State Machine Safety Property，亦即是，当任一机器 apply 了某一特定 log entry 到其 state machine 中，则其余服务器都不可能 apply 了一个 index 相同但 command 不同的 log。

差不多依据上述划分，6.824 中 Raft 的实现指导逻辑还是挺清晰的，其中 safety property 由 Leader election 和 Log replication 共同承担，并且将 Persistence 作为最后一部分。实现过程主要分为：
- Leader Election and Heartbeats：首先令 Raft 能够在不存在故障的情景下选举出一个 leader，并且稳定保持状态；
- Log Replication：其次令 Raft 能够保持一个 consistent 并且 replicated 的 log；
- Persistence：最后令 Raft 能够持久化保存 persistent state，这样在重启后可以进行恢复。

其中，本文主要参考 Raft paper，其中的 **figure 2** 作用很大。本文实现**大量依赖 channel 实现消息传递和线程同步**。仍然存在**少量 bug**，主要是 fail to pass unreliable test case，有待继续优化（主要是分布式调试无从下手）。

[Code Link](https://github.com/Wiesen/MIT-6.824/blob/master/2016/raft/raft.go)

Leader Election and Heartbeat
---
实现 Leader Election，主要是需要完成以下三个功能：

1. Role Transfer：state machine & election timer
 首先实现 Raft 的 sate machine，所有 server 都应当在初始化 Raft peer 时开启一个单独的线程来维护状态，令其在 Follower，Candidate，Leader 三个状态之间进行转换。

 有一点**注意**的是，`nextIndex[]` 和 `matchindex[]` 需要在 election 后进行 reinitialize。

        func (rf *Raft) changeRole(role Role) {
        	rf.mu.Lock()
        	defer rf.mu.Unlock()
        	rf.role = role
        	switch rf.role {
        	case Leader:
        		rf.reinitialize()    // reinitialized after election
        		go rf.runAsLeader()
        	case Candidate:
        		go rf.runAsCandidate()
        	case Follower:
        		go rf.runAsFollower()
        	}
        }

 此外，定时器 election timer 的作用十分关键，所有 server 都应当在初始化 Raft peer 时开启一个单独的线程来维护 election timer。
 
 当 election timer 超时时，机器将会转换至 Candidate 状态。另外在以下三种情况下将会 reset 定时器：收到合法的 heartbeat message；投票给除自身以外的 candidate；自身启动election（本文视为转换为 Candidate 状态）。
 
 同样有一点需要注意，需要确保不同机器上的 timer 异步，也就是不会同时触发，否则所有机器都会自投票导致无法选举 leader。在 golang 中通过以时间作为种子投入到随机发生器中：`rand.Seed(time.Now().UnixNano())`

        func (rf *Raft) startElectTimer() {
        	floatInterval := int(RaftElectionTimeoutHigh - RaftElectionTimeoutLow)
        	timeout := time.Duration(rand.Intn(floatInterval)) + RaftElectionTimeoutLow
        	electTimer := time.NewTimer(timeout)
        	for {
        		select {
        		case <- rf.chanHeartbeat: // received valid heartbeat message
        			rf.resetElectTimer(electTimer)
        		case <- rf.chanGrantVote: // voted for other server 
        			rf.resetElectTimer(electTimer)
        		case <-electTimer.C:      // fired election  
        			rf.chanRole <- Candidate
        			rf.resetElectTimer(electTimer)
        		}
        	}
        }
        
2. RequstVote RPC 的 sender 和 handler

 完成 Raft 的 sate machine后，开始实现 Raft 中的 RequestVote 操作，使得能够选举出一个 leader。

 机器处于 Candidate 状态时应当启动 election：`currentTerm++` -> `votedFor = me` -> `sendRequestVote()`。其中 `sendRequestVote()` 应当异步，也就是并发给其余机器发送 RequstVote。
 
 当出现以下情况，当前 election 过程终结：
 - 获得大多数机器的投票 -> 转换为 Leader 状态；
 - 接受到的 reply 中 `term T > currentTerm` -> 更新 `currentTerm`，转换为 Follower 状态；
 - election timer 超时 -> 终结当前 election，并且启动新一轮 election，保持 Candidate 状态；
 
 一个机器接收到 RequstVote RPC，需要决定是否投票。
 - 如果 RequstVote RPC 中的 `term T < currentTerm`，直接返回 false 拒绝投票即可；
 - 如果 RequstVote RPC 中的 `term T > currentTerm`，则需要更新 `currentTerm`，转换为 Follower 状态；
 - 如果该机器在 `currentTerm` 已经投票，则直接返回 false 拒绝投票；
 - 否则在满足 "Candidate's log is at least as up-to-date as receiver's log" 时返回 true 投票，并且 reset election timeout；

 所谓 **“up-to-date”** 简单来说就是：比较两个 log 中的最后一条 entry 的 `index` 和 `term`。
 - 当两个 log 的最后一条 entry 的 term 不同，则 **later** term is more up-to-date；
 - 当两个 log 的最后一条 entry 的 term 相同，则 whichever log is **longer** is more up-to-date

		// RequestVote RPC handler.
		func (rf *Raft) RequestVote(args RequestVoteArgs, reply *RequestVoteReply) {
		  if rf.currentTerm > args.Term {
		  	reply.Term = rf.currentTerm
		  		reply.VoteGranted = false
		  	return
		  }
		  if rf.currentTerm < args.Term {
		  	rf.currentTerm = args.Term
		  	rf.votedFor = -1
		  	rf.chanRole <- Follower
		  }
		  reply.Term = args.Term
		  if rf.votedFor != -1 && rf.votedFor != args.CandidateId {
		  	reply.VoteGranted = false
		  } else if rf.logTable[len(rf.logTable)-1].Term > args.LastLogTerm {
		  	reply.VoteGranted = false 	//! different term
		  } else if len(rf.logTable)-1 > args.LastLogIndex && rf.logTable[len(rf.logTable)-1].Term == args.LastLogTerm { 
		  	reply.VoteGranted = false   //! same term but different index
		  }else {
		  	reply.VoteGranted = true
		  	rf.votedFor = args.CandidateId
		  	rf.chanGrantVote <- true
		  }
		}

3. Heartbeat 的 sender 和 handler

 最后实现 Raft 中的 Heartbeat 操作，能够令 Leader 稳定保持状态。
 
 机器处于 Leader 状态时应当启动 heartbeat，而其实际上就是不含 log entry 的 AppendEntries（只需要检测某一机器的 log 是否最新）。
 
 其中 `sendHeartbeat()` 应当异步，也就是并发给其余机器发送 heartbeat。每一个 heartbeat 线程利用 timer 来周期性地触发操作。

        func (rf *Raft) sendHeartbeat(server int) {
            for rf.getRole() == Leader {
      				rf.doAppendEntries(server)  // later explain in Log Replication
      				heartbeatTimer.Reset(RaftHeartbeatPeriod)
      				<-heartbeatTimer.C
	    	    }
        }
 
 一个机器接收到不含 log entry 的 AppendEntries RPC（也就是 heartbeat）时，需要决定是否更新自身的 term 和 leaderId：
 - 如果 AppendEntries RPC 中的 `term T < currentTerm`，则 reply 中返回 `currentTerm` 让发送方更新；
 - 如果 RequstVote RPC 中的 `term T > currentTerm`，则需要更新 `currentTerm` 和 `leaderId`，转换为 Follower 状态，并且 reset election timeout；
 
            // temporary AppendEntries RPC handler.
            func (rf *Raft) AppendEntries(args AppendEntriesArgs, reply *AppendEntriesReply) {
            if args.Term < rf.currentTerm {
            	reply.Term = rf.currentTerm
            	reply.Success = false
            	return
            }
            //! update current term and only one leader granted in one term
            if rf.currentTerm < args.Term {
            	rf.currentTerm = args.Term
            	rf.votedFor = -1
            	rf.leaderId = args.LeaderId
            	rf.chanRole <- Follower
            }
            rf.chanHeartbeat <- true
            reply.Term = args.Term
            }

Log Replication
---
完成了 Leader election 后，下一步是令 Raft 保持一个 consistent 并且 replicated 的 log。

1. AppendEntries RPC sender 

 在 Raft 中，只有 leader 允许添加 log，并且通过发送含有log 的 AppendEntries RPC 给其余机器令其 log 保持一致。
 
 除了 heartbeat 时会发送 AppendEntries RPC 外，当 Leader 收到 client 的 Request 后也有进行 replica 操作，同样是异步并发提升性能。

        func (rf *Raft) Replica() {
        // replica log
        for server := range rf.peers {
        	if server != rf.me {
        		go rf.doAppendEntries(server)
        	}
        }

 接下来主要是 `doAppendEntries(server int)` 的实现细节。
 
 当 Leader 的 log 比某 server 长时，亦即是 `len(rf.logTable) - 1 >= rf.nextIndex[server]`，则需要发送 entry，并且直到该 server's log has caught up leader's log。
 
 有一点要**注意**的是：Leader 在对某 server 进行上述的连续发送时间或者等待 reply 的时间可能会大于 heartbeat timeout，因此触发 AppendEntries RPC；或者在上述时间内 client 向 Leader 提交了新的 log，这时候再次触发了 AppendEntries RPC。然而 Leader 本身已经在向该 server 发送 AppendEntries RPC，这时候对该 server 进行 heartbeat 是多余的。 
 
 本文检测 Leader 是否在向某 server 发送 AppendEntries RPC，若是则直接退出不再重复操作。这里设计为 Leader 为每一 server 维护一个带有**一个缓存**的 channel，要对某 server 进行 AppendEntries RPC 必须先 read channel。
 
 最后如果 RPC failed 则直接退出，否则收到 AppendEntries RPC 的 reply 时：
  - 首先判断是否 `reply.Term > args.Term`，若是则 Leader 需要更新自身 currentTerm，并且转换为 Follower 状态。
  - 接着如果 args 中带有 log entry，则 `reply.Success` 会指示是否添加成功。如果成功则更新 `rf.matchIndex[server]` 和 `rf.nextIndex[server]`；否则 `rf.nextIndex[server]--` 直至等于 `rf.matchIndex[server]`（后续优化）。

            func (rf *Raft) doAppendEntries(server int) {
            	select {
            	case <- rf.chanAppend[server]:
            	default:return
            	}
            	isAppend := true
            	for isAppend && rf.getRole() == Leader {
            		var args AppendEntriesArgs
            		// Set the basic arguments of arg
                    ...
            		if (len(rf.logTable) - 1 >= rf.nextIndex[server]) {
            			args.Entries = append(args.Entries, rf.logTable[rf.nextIndex[server]])
            		} else {
            			isAppend = false
            		}
            		// Copy with reply
            		var reply AppendEntriesReply
            		if rf.sendAppendEntries(server, args, &reply) {
            			if reply.Term > args.Term {
            				rf.currentTerm = args.Term
                    		rf.votedFor = -1
            				rf.chanRole <- Follower
            				break
            			}
            			if isAppend {
            				if reply.Success {
            					rf.matchIndex[server] = rf.nextIndex[server]
            					rf.nextIndex[server]++
            				} else {
                                // later explain in AppendEntries RPC optimization
            					if reply.FailIndex > rf.matchIndex[server] {
            					  rf.nextIndex[server] = reply.FailIndex
            					} else {
            						rf.nextIndex[server] = rf.matchIndex[server] + 1
            					}     
            				}
            			}
            		} else {
            			break
            		}
            	}
            	rf.chanAppend[server] <- true
            }

2. AppendEntries RPC handler

 一个机器接收到含有 log entry 的 AppendEntries RPC，前一部分跟处理 heartbeat 一样，剩下的部分则是决定是否更新自身的 log。
 
 剩下根据 Raft paper 中的 figure 2 按部就班实现就好了 【好像并没有什么值得特别提的……

3. AppendEntries RPC optimization

 这一部分主要是优化 AppendEntries RPC，当发生拒绝 append 时减少 master 需要发送 RPC 的次数。虽然 Raft paper 中怀疑这个优化操作是否有必要，但 lab2 中有些 unreliable test case 偏偏需要你实现这个优化操作……
 
 首先需要在 struct AppendEntriesReply 中添加两个数据：`FailIndex int, FailTerm int`，分别用于指示 conflicting entry 所在 term 以及在该 term 中存储的 first index；
 
 在 handler 的优化如下：如果 log unmatched，首先记录 `FailTerm`，然后从当前 conflicting entry 开始向前扫描，直到到达前一个 term，记录下 `FailIndex`；
 
 在 sender 的优化如下：当 append 失败时，如果 `FailIndex > matchIndex[server]`，则 `nextIndex[server]` 直接退至 `FailIndex`，减少了需要尝试 append 的 RPC 次数；否则 `nextIndex[server]` 退回到 `matchIndex[server] + 1`。

4. Apply committed entries to local service replica

 对于 Leader 而言，需要不断检测当前 term 的 log entry 的 replica 操作是否完成，然后进行 commit 操作。
 
 当超过半数的机器已经完成 replica 操作，则 Leader 认为该条 log entry 可以 commit。
 
 一旦当前 term 的某条 log entry L 是通过上述方式 commit 的，则根据 Raft 的 **Log Matching Property**，Leader 可以 commit 先于 L 添加到 log 的所有 entry。

        func (rf *Raft) updateLeaderCommit() {
        	oldIndex := rf.getCommitIndex()
        	newIndex := oldIndex
        	for i := len(rf.logTable)-1; i>oldIndex && rf.logTable[i].Term==rf.getCurrentTerm(); i-- {
        		countServer := 1
        		for server := range rf.peers {
        			if server != rf.me && rf.matchIndex[server] >= i {
        				countServer++
        			}
        		}
        		if countServer > len(rf.peers) / 2 {
        			newIndex = i
        			break
        		}
        	}
        	if oldIndex == newIndex {
        		return
        	}
        	//! update the log added in previous term
        	for i := oldIndex + 1; i <= newIndex; i++ {
        		rf.chanCommitted <- ApplyMsg{Index:i, Command:rf.logTable[i].Command}
        	}
        	rf.setCommitIndex(newIndex)
        }
 
 对于 Follower 而言，只需要根据 AppendEntries RPC 中的 `leaderCommit` 值及其自身的 `commitIndex` 值，然后 commit 之中的 entry 即可。 

Persistence
---
最后一步是持久化保存 persistent state，不过在 lab2 仅是通过 `persister` object 来保存，并没有真正使用到磁盘。实现了这一部分后可以稳定地 pass 掉关于 Persist 的 test case。

1. Read persist

 在 Make Raft peer 时读取 persister。

2. Write persist

 修改了 Raft 的 persistent state 后应当及时写至 persister，主要是在以下几个地方插入 write：
  - 启动了 election 修改了自身的 persistent state后；
  - 受到了 RequestVote RPC 和 AppendEntries RPC 的 reply，得知需要更新自身的 persistent state 后；
  -  RequestVote RPC handler 和 AppendEntries RPC handler 处理完毕后；
  -  Leader 收到 client 的请求命令，添加到自身的 log 后。

Defect
---
本文还有些不足，有待后续优化。主要是**不能稳定地** pass `TestUnreliableAgree()` + **不能** pass `TestFigure8Unreliable()`

To Be Continue...
