<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wiesen&#39;s Blog</title>
    <link>http://wiesen.github.io/</link>
    <description>Recent content on Wiesen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016. All rights reserved.</copyright>
    <lastBuildDate>Tue, 16 Aug 2016 17:32:46 +0800</lastBuildDate>
    <atom:link href="http://wiesen.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Robotics: Estimation and learning</title>
      <link>http://wiesen.github.io/post/Robotics-Estimation-and-Learning/</link>
      <pubDate>Tue, 16 Aug 2016 17:32:46 +0800</pubDate>
      
      <guid>http://wiesen.github.io/post/Robotics-Estimation-and-Learning/</guid>
      <description>&lt;p&gt;One of the course of Robotics in Coursera from the University of Pennsylvania. It is helpful to learn the classical algorithm of robotics.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.coursera.org/learn/robotics-learning&#34;&gt;Course Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here are some notes in Chinese collected from Zhihu.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Gaussian Model Learning: &lt;a href=&#34;https://zhuanlan.zhihu.com/p/21648507?refer=robotics-learning&#34;&gt;从高斯分布、机器人误差、EM 算法到小球检测&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bayesian Estimation - Target Tracking: &lt;a href=&#34;https://zhuanlan.zhihu.com/p/21692854?refer=robotics-learning&#34;&gt;目标追踪之卡尔曼滤波&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mapping: &lt;a href=&#34;https://zhuanlan.zhihu.com/p/21738718?refer=robotics-learning&#34;&gt;占据栅格地图（Occupancy Grid Map）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bayesian Estimation - Localization: &lt;a href=&#34;https://zhuanlan.zhihu.com/p/21974439&#34;&gt;蒙特卡罗定位（Particle Filter Localization）&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>MIT 6.824 lab3 Fault-Tolerant Key/Value Service Implementation</title>
      <link>http://wiesen.github.io/post/MIT-6.824-lab3-Fault-Tolerant-KVService-Implementation/</link>
      <pubDate>Sat, 06 Aug 2016 17:32:46 +0800</pubDate>
      
      <guid>http://wiesen.github.io/post/MIT-6.824-lab3-Fault-Tolerant-KVService-Implementation/</guid>
      <description>

&lt;p&gt;lab3 是基于 lab2 实现的 Raft Consensus Algorithm 之上实现 KV Service。主要分为两部分：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Part A：Key/value service without log compaction，即实现基本的分布式存储服务。&lt;/li&gt;
&lt;li&gt;Part B: Key/value service with log compaction，即在 Part A 基础上实现 log compaction。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;代码分别可以 pass 各个 test case，但所有一起跑时有时会卡在 &lt;code&gt;TestPersistPartition()&lt;/code&gt; 这里，初步猜测是 raft 的实现还有点 bug。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Wiesen/MIT-6.824/tree/master/2016/kvraft&#34;&gt;Code Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;part-a-key-value-service&#34;&gt;Part A：Key/value service&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;KV Database Client API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;key/value database 的 client API 必须满足以下要求：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;保证仅执行一次(at most once semantics)：API 必须为每个 Client 及 每个 Request 赋予唯一的 id；&lt;/li&gt;
&lt;li&gt;必须向使用该 API 的应用提供 sequential consistency：对于每个 Client，仅有一条 Request RPC 在显式执行(利用 lock 实现)；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此外，API 应当一直尝试向 key/value server 发起 RPC 直到收到 positive reply；并且记住 leader id，从而尽可能避免失败次数。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Raft Handler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Raft Server Handler 需要满足以下要求：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;保证每个 client 的每条 request (主要是写操作) 仅执行一次：对于重复的 request，仅向 client 回复操作结果；&lt;/p&gt;

&lt;p&gt;本文为每个 client 维护一个已执行的最大 requestId 值 (&lt;code&gt;map[int64]int&lt;/code&gt;)，从而检测并过滤重复的 request&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; func (kv *RaftKV) isDuplicated(clientId int64, requestId int) bool {
    kv.mu.Lock()
    defer kv.mu.Unlock()
    if value, ok := kv.ack[clientId]; ok &amp;amp;&amp;amp; value &amp;gt;= requestId {
        return true
    }
    kv.ack[clientId] = requestId
    return false
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在向 raft 添加等待结果的同时，需要一直监听返回管道 &lt;code&gt;applyCh&lt;/code&gt; ，以接收已经达成一致的 entry；&lt;/p&gt;

&lt;p&gt;我们开一个 goroutine &lt;code&gt;update()&lt;/code&gt; 一直监听 &lt;code&gt;applyCh&lt;/code&gt;，并且基于 entry 的 index 各维护一个管道 (&lt;code&gt;map[int]chan Result&lt;/code&gt;)，存放在 raft servers 间达成一致的 entry，等待 handler 的读取或者直接丢弃。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; func (kv *RaftKV) Update() {   // ignore snapshot
    for true {
        msg := &amp;lt;- kv.applyCh
        request := msg.Command.(Op)
        var result Result
        ...                     // set variable value
        result.reply = kv.Apply(request, kv.isDuplicated(clientId, requestId))
        kv.sendResult(msg.Index, result);
    }
  }

 func (kv *RaftKV) sendResult(index int, result Result) {
    kv.mu.Lock()
    defer kv.mu.Unlock()
    if _, ok := kv.messages[index]; !ok {
        kv.messages[index] = make(chan Result, 1)
    } else {
        select {
        case &amp;lt;- kv.messages[index]:
        default:
        }
    }
    kv.messages[index] &amp;lt;- result
 }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;key/value server 向 raft 添加一个 entry 后，阻塞读取其 index 对应的管道，直到接收到结果或者超时（本文设置为 1s）。当接收到的结果 clientId 或者 requestId 不一致时，表明 leader 已经发生了更替，由 Client 重新向 server 发起 RPC。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;有一个需要注意的地方是，&lt;strong&gt;必须利用 &lt;code&gt;gob.Register()&lt;/code&gt; 注册需要通过 RPC 发送的结构体&lt;/strong&gt;，这样结构体才能够被解析，否则发送过去就是一个 &lt;code&gt;nil&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;part-b-log-compaction&#34;&gt;Part B: log compaction&lt;/h2&gt;

&lt;p&gt;随着运行时间的增加，raft server 的 log table 会越来越大，不仅会占用越多空间，而且一旦出现宕机则 replay 也需要越长时间。比如不加以管理，则势必影响服务的可用性。为了令其维持合理长度不至于无限增加，必须在适当的时候抛弃旧的 log entries。&lt;/p&gt;

&lt;p&gt;这部分主要有以下实现点：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Take snapshots independently&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里各个 raft server 各自独立进行 snapshot，而这并不会影响 raft 的一致性。因为数据始终从 leader 流向 follower，各个 raft server 只是将数据重新组织而已。&lt;/p&gt;

&lt;p&gt;这部分主要解决的问题是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;(When)&lt;/strong&gt; 什么时候进行 snapshot：由 key/value server 检测所连接的 raft server 存储大小是否即将超过阈值，然后通知该 raft server 进行 snapshot；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(What)&lt;/strong&gt; 在 snapshot 中存储什么信息：如 raft-extended paper Figure 12 里的描述，主要包括当前 &lt;code&gt;LastIncludedIndex&lt;/code&gt;, &lt;code&gt;LastIncludedTerm&lt;/code&gt; 以及 &lt;code&gt;data&lt;/code&gt;，其中 &lt;code&gt;data&lt;/code&gt; 是状态机状态，来自于 key/value server。一点需要注意的是，必须保存用于检测 duplicate client requests 的数据，因此 &lt;code&gt;data&lt;/code&gt; 在这里包括整个数据库和检测重复的 map；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;(How)&lt;/strong&gt; 如何令 raft 在仅有最新一部分 log 的情况下保持正常运行：主要是将 &lt;code&gt;rf.logTable[0]&lt;/code&gt; 作为起始 entry，一旦有 follower 请求更老的 entries，则应该发送 InstallSnapshot RPC；
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要增加和修改的地方如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;update()&lt;/code&gt;： 在每次接收到 raft server 返回的 entry 时，检测 raft state size，若临近阈值则将封装 &lt;code&gt;data&lt;/code&gt;，然后通知 raft 从 &lt;code&gt;entry.index&lt;/code&gt; 开始 snapshotting；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;StartSnapshot(data []byte, index int)&lt;/code&gt;：删除 log table 中序号小于 &lt;code&gt;index&lt;/code&gt; 的 entries，并且在 &lt;code&gt;data&lt;/code&gt; 后添加最后包括的 entry 的相关数据，最后持久化保存；&lt;/li&gt;
&lt;li&gt;其余琐碎并且细节的修改还包括：对 entry 增加一个字段 &lt;code&gt;index&lt;/code&gt;，记录其在所在 &lt;code&gt;term&lt;/code&gt; 的序号，在需要对 log table 进行操作地方（尤其是涉及对 entry 在 table 中的位置），注意 log table 中的起始序号：&lt;code&gt;baseLogIndex := rf.logTable[0].Index&lt;/code&gt; 再进行处理；&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;InstallSnapshot RPC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;当一个 follower 远远落后于 leader（即无法在 log table 中找到匹配的 entry），则应该发送 InstallSnapshot RPC。我们参考 raft-extended paper Figure 13 设置数据结构，但有所简化：由于 lab 里的 Snapshot 不大，因此没有设置 chunk，也就是不需要对 snapshot 进行切分。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;InstallSnapshot RPC sender&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;当 heartbeat timeout 时 leader 检测 follower 的 log table 是否与自身一致，如果 &lt;code&gt;rf.nextIndex[server] &amp;lt;= baseLogIndex&lt;/code&gt;，则 leader 应该向 follower &lt;code&gt;server&lt;/code&gt; 发送 InstallSnapshot RPC,否则 &lt;code&gt;AppendEntries&lt;/code&gt;。这里发送设置好相应的参数即可，跟发送 &lt;code&gt;AppendEntries&lt;/code&gt; 差不多，收到正确的 reply 则 更新 &lt;code&gt;matchIndex&lt;/code&gt; 和 &lt;code&gt;nextIndex&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; baseLogIndex := rf.logTable[0].Index
 if rf.nextIndex[server] &amp;lt;= baseLogIndex {
    ...             // set variable value
    var reply InstallSnapshotReply
    if rf.sendInstallSnapshot(server, args, &amp;amp;reply) {
        if rf.role != Leader {
            return
        }
        if args.Term != rf.currentTerm || reply.Term &amp;gt; args.Term {
            if reply.Term &amp;gt; args.Term {
                ... // update term and role
            }
            return
        }
        rf.matchIndex[server] = baseLogIndex
        rf.nextIndex[server] = baseLogIndex + 1
        return
    }
 }
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;InstallSnapshot RPC handler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;由于没有设置 chunk，所以这里的 handler 比 paper 中的 implementation 简化不少：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先检测 &lt;code&gt;args.Term &amp;lt; rf.currentTerm&lt;/code&gt;，满足则直接返回自身 term 即可；&lt;/li&gt;
&lt;li&gt;其次检测 &lt;code&gt;rf.currentTerm &amp;lt; args.Term&lt;/code&gt;,满足则更新自身 term 和 role;&lt;/li&gt;
&lt;li&gt;然后更新自身的 log table，把 LastIncluded entry 前的丢弃；&lt;/li&gt;
&lt;li&gt;最后将 snapshot 发送给 key/value server，以重置数据库数据，更新状态。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;最后&#34;&gt;最后&lt;/h2&gt;

&lt;p&gt;写 lab3 的时候，对 lab2 的代码调整很大，发现了不少 bug 和逻辑不合理的地方，整体上对 raft 的理解更加深入了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>http://wiesen.github.io/about/</link>
      <pubDate>Sun, 19 Jun 2016 19:20:15 +0800</pubDate>
      
      <guid>http://wiesen.github.io/about/</guid>
      <description>

&lt;h2 id=&#34;wiesen-yang&#34;&gt;Wiesen Yang&lt;/h2&gt;

&lt;p&gt;2015&amp;ndash;Present, MS in School of Computer Science and Technology, ZJU, Expected 2018&lt;/p&gt;

&lt;p&gt;2011&amp;ndash;2015, BA in Internet of Things Engineering, College of Information Science and Engineering, NEU&lt;/p&gt;

&lt;p&gt;Dedicated in distributed system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MIT 6.824 lab2 Raft Consensus Algorithm Implementation</title>
      <link>http://wiesen.github.io/post/MIT-6.824-lab2-Raft-Consensus-Algorithm-Implementation/</link>
      <pubDate>Fri, 10 Jun 2016 21:32:46 +0800</pubDate>
      
      <guid>http://wiesen.github.io/post/MIT-6.824-lab2-Raft-Consensus-Algorithm-Implementation/</guid>
      <description>

&lt;p&gt;Raft 将一致性问题分为了三个相对独立的子问题，分别是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Leader election&lt;/strong&gt;：当前 leader 崩溃时，集群中必须选举出一个新的 leader；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Log replication&lt;/strong&gt;：leader 必须接受来自 clients 的 log entries，并且将其 replicate 到集群机器中，强制其余 logs 与其保持一致；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Safety&lt;/strong&gt;：Raft 中最关键的 safety property 是 State Machine Safety Property，亦即是，当任一机器 apply 了某一特定 log entry 到其 state machine 中，则其余服务器都不可能 apply 了一个 index 相同但 command 不同的 log。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;差不多依据上述划分，6.824 中 Raft 的实现指导逻辑还是挺清晰的，其中 safety property 由 Leader election 和 Log replication 共同承担，并且将 Persistence 作为最后一部分。实现过程主要分为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Leader Election and Heartbeats：首先令 Raft 能够在不存在故障的情景下选举出一个 leader，并且稳定保持状态；&lt;/li&gt;
&lt;li&gt;Log Replication：其次令 Raft 能够保持一个 consistent 并且 replicated 的 log；&lt;/li&gt;
&lt;li&gt;Persistence：最后令 Raft 能够持久化保存 persistent state，这样在重启后可以进行恢复。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中，本文主要参考 Raft paper，其中的 &lt;strong&gt;figure 2&lt;/strong&gt; 作用很大。本文实现&lt;strong&gt;大量依赖 channel 实现消息传递和线程同步&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/Wiesen/MIT-6.824/blob/master/2016/raft/raft.go&#34;&gt;Code Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;leader-election-and-heartbeat&#34;&gt;Leader Election and Heartbeat&lt;/h2&gt;

&lt;p&gt;实现 Leader Election，主要是需要完成以下三个功能：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Role Transfer：state machine &amp;amp; election timer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;首先实现 Raft 的 sate machine，所有 server 都应当在初始化 Raft peer 时开启一个单独的线程来维护状态，令其在 Follower，Candidate，Leader 三个状态之间进行转换。&lt;/p&gt;

&lt;p&gt;有一点&lt;strong&gt;注意&lt;/strong&gt;的是，&lt;code&gt;nextIndex[]&lt;/code&gt; 和 &lt;code&gt;matchindex[]&lt;/code&gt; 需要在 election 后进行 reinitialize。&lt;/p&gt;

&lt;p&gt;func (rf *Raft) changeRole() {
        for true {
            switch rf.role {
            case Leader:
                for i := range rf.peers {
                    rf.nextIndex[i] = rf.logTable[len(rf.logTable)-1].Index + 1
                    rf.matchIndex[i] = 0
                }
                go rf.doHeartbeat()
                &amp;lt;-rf.chanRole
            case Candidate:
                chanQuitElect := make(chan bool)
                go rf.startElection(chanQuitElect)
                &amp;lt;-rf.chanRole
                close(chanQuitElect)
            case Follower:
                &amp;lt;-rf.chanRole
            }
        }
    }&lt;/p&gt;

&lt;p&gt;此外，定时器 election timer 的作用十分关键，所有 server 都应当在初始化 Raft peer 时开启一个单独的线程来维护 election timer。&lt;/p&gt;

&lt;p&gt;当 election timer 超时时，机器将会转换至 Candidate 状态。另外在以下三种情况下将会 reset 定时器：收到合法的 heartbeat message；投票给除自身以外的 candidate；自身启动election（本文视为转换为 Candidate 状态）。&lt;/p&gt;

&lt;p&gt;同样有一点需要注意，需要确保不同机器上的 timer 异步，也就是不会同时触发，否则所有机器都会自投票导致无法选举 leader。在 golang 中通过以时间作为种子投入到随机发生器中：&lt;code&gt;rand.Seed(time.Now().UnixNano())&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rf *Raft) startElectTimer() {
    floatInterval := int(RaftElectionTimeoutHigh - RaftElectionTimeoutLow)
    timeout := time.Duration(rand.Intn(floatInterval)) + RaftElectionTimeoutLow
    electTimer := time.NewTimer(timeout)
    for {
        select {
        case &amp;lt;- rf.chanHeartbeat: // received valid heartbeat message
            rf.resetElectTimer(electTimer)
        case &amp;lt;- rf.chanGrantVote: // voted for other server 
            rf.resetElectTimer(electTimer)
        case &amp;lt;-electTimer.C:      // fired election  
            rf.chanRole &amp;lt;- Candidate
            rf.resetElectTimer(electTimer)
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;RequstVote RPC 的 sender 和 handler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;完成 Raft 的 sate machine后，开始实现 Raft 中的 RequestVote 操作，使得能够选举出一个 leader。&lt;/p&gt;

&lt;p&gt;机器处于 Candidate 状态时应当启动 election：&lt;code&gt;currentTerm++&lt;/code&gt; -&amp;gt; &lt;code&gt;votedFor = me&lt;/code&gt; -&amp;gt; &lt;code&gt;sendRequestVote()&lt;/code&gt;。其中 &lt;code&gt;sendRequestVote()&lt;/code&gt; 应当异步，也就是并发给其余机器发送 RequstVote。&lt;/p&gt;

&lt;p&gt;当出现以下情况，当前 election 过程终结：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获得大多数机器的投票 -&amp;gt; 转换为 Leader 状态；&lt;/li&gt;
&lt;li&gt;接受到的 reply 中 &lt;code&gt;term T &amp;gt; currentTerm&lt;/code&gt; -&amp;gt; 更新 &lt;code&gt;currentTerm&lt;/code&gt;，转换为 Follower 状态；&lt;/li&gt;
&lt;li&gt;election timer 超时 -&amp;gt; 终结当前 election，并且启动新一轮 election，保持 Candidate 状态；
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一个机器接收到 RequstVote RPC，需要决定是否投票：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果 RequstVote RPC 中的 &lt;code&gt;term T &amp;lt; currentTerm&lt;/code&gt;，直接返回 false 拒绝投票即可；&lt;/li&gt;
&lt;li&gt;如果 RequstVote RPC 中的 &lt;code&gt;term T &amp;gt; currentTerm&lt;/code&gt;，则需要更新 &lt;code&gt;currentTerm&lt;/code&gt;，转换为 Follower 状态；&lt;/li&gt;
&lt;li&gt;如果该机器在 &lt;code&gt;currentTerm&lt;/code&gt; 已经投票，则直接返回 false 拒绝投票；&lt;/li&gt;
&lt;li&gt;否则在满足 &amp;ldquo;Candidate&amp;rsquo;s log is at least as up-to-date as receiver&amp;rsquo;s log&amp;rdquo; 时返回 true 投票，并且 reset election timeout；
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所谓 &lt;strong&gt;“up-to-date”&lt;/strong&gt; 简单来说就是：比较两个 log 中的最后一条 entry 的 &lt;code&gt;index&lt;/code&gt; 和 &lt;code&gt;term&lt;/code&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当两个 log 的最后一条 entry 的 term 不同，则 &lt;strong&gt;later&lt;/strong&gt; term is more up-to-date；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;当两个 log 的最后一条 entry 的 term 相同，则 whichever log is &lt;strong&gt;longer&lt;/strong&gt; is more up-to-date&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// RequestVote RPC handler.
func (rf *Raft) RequestVote(args RequestVoteArgs, reply *RequestVoteReply) {
  if rf.currentTerm &amp;gt; args.Term {
    reply.Term = rf.currentTerm
        reply.VoteGranted = false
    return
  }
  if rf.currentTerm &amp;lt; args.Term {
    rf.currentTerm = args.Term
    rf.votedFor = -1
    rf.chanRole &amp;lt;- Follower
  }
  reply.Term = args.Term
  if rf.votedFor != -1 &amp;amp;&amp;amp; rf.votedFor != args.CandidateId {
    reply.VoteGranted = false
  } else if rf.logTable[len(rf.logTable)-1].Term &amp;gt; args.LastLogTerm {
    reply.VoteGranted = false   //! different term
  } else if len(rf.logTable)-1 &amp;gt; args.LastLogIndex &amp;amp;&amp;amp; rf.logTable[len(rf.logTable)-1].Term == args.LastLogTerm { 
    reply.VoteGranted = false   //! same term but different index
  }else {
    reply.VoteGranted = true
    rf.votedFor = args.CandidateId
    rf.chanGrantVote &amp;lt;- true
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Heartbeat 的 sender 和 handler&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;最后实现 Raft 中的 Heartbeat 操作，能够令 Leader 稳定保持状态。&lt;/p&gt;

&lt;p&gt;机器处于 Leader 状态时应当启动 heartbeat，而其实际上就是不含 log entry 的 AppendEntries（只需要检测某一机器的 log 是否最新）。&lt;/p&gt;

&lt;p&gt;其中 &lt;code&gt;sendHeartbeat()&lt;/code&gt; 应当异步，也就是并发给其余机器发送 heartbeat。每一个 heartbeat 线程利用 timer 来周期性地触发操作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rf *Raft) sendHeartbeat(server int) {
    for rf.getRole() == Leader {
                rf.doAppendEntries(server)  // later explain in Log Replication
                heartbeatTimer.Reset(RaftHeartbeatPeriod)
                &amp;lt;-heartbeatTimer.C
        }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一个机器接收到不含 log entry 的 AppendEntries RPC（也就是 heartbeat）时，需要决定是否更新自身的 term 和 leaderId：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果 AppendEntries RPC 中的 &lt;code&gt;term T &amp;lt; currentTerm&lt;/code&gt;，则 reply 中返回 &lt;code&gt;currentTerm&lt;/code&gt; 让发送方更新；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;如果 RequstVote RPC 中的 &lt;code&gt;term T &amp;gt; currentTerm&lt;/code&gt;，则需要更新 &lt;code&gt;currentTerm&lt;/code&gt; 和 &lt;code&gt;leaderId&lt;/code&gt;，转换为 Follower 状态，并且 reset election timeout；&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// temporary AppendEntries RPC handler.
func (rf *Raft) AppendEntries(args AppendEntriesArgs, reply *AppendEntriesReply) {
    if args.Term &amp;lt; rf.currentTerm {
        reply.Term = rf.currentTerm
        reply.Success = false
        return
    }
    //! update current term and only one leader granted in one term
    if rf.currentTerm &amp;lt; args.Term {
        rf.currentTerm = args.Term
        rf.votedFor = -1
        rf.leaderId = args.LeaderId
        rf.chanRole &amp;lt;- Follower
    }
    rf.chanHeartbeat &amp;lt;- true
    reply.Term = args.Term
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;log-replication&#34;&gt;Log Replication&lt;/h2&gt;

&lt;p&gt;完成了 Leader election 后，下一步是令 Raft 保持一个 consistent 并且 replicated 的 log。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;AppendEntries RPC sender&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在 Raft 中，只有 leader 允许添加 log，并且通过发送含有 log 的 AppendEntries RPC 给其余机器令其 log 保持一致。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rf *Raft) Replica() {
// replica log
for server := range rf.peers {
    if server != rf.me {
        go rf.doAppendEntries(server)
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;接下来主要是 &lt;code&gt;doAppendEntries(server int)&lt;/code&gt; 的实现细节。&lt;/p&gt;

&lt;p&gt;当 Leader 的 log 比某 server 长时，亦即是 &lt;code&gt;rf.nextIndex[server] &amp;lt; len(rf.logTable)&lt;/code&gt;，则需要发送 entry&lt;/p&gt;

&lt;p&gt;有一点要&lt;strong&gt;注意&lt;/strong&gt;的是：Leader 在对某 server 进行上述的连续发送时间或者等待 reply 的时间可能会大于 heartbeat timeout，因此触发 AppendEntries RPC。然而 Leader 本身正在等待 reply，这时候重复发送是多余的。&lt;/p&gt;

&lt;p&gt;本文检测 Leader 是否在向某 server 发送 AppendEntries RPC，若是则直接退出不再重复操作：这里设计为 Leader 为每一 server 维护一个带有&lt;strong&gt;一个缓存&lt;/strong&gt;的管道，这样某 server 进行 AppendEntries RPC 前可以确认是否正在处理。&lt;/p&gt;

&lt;p&gt;最后如果 RPC failed 则直接退出，否则收到 AppendEntries RPC 的 reply 时：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;首先判断是否 &lt;code&gt;reply.Term &amp;gt; args.Term&lt;/code&gt;，若是则 Leader 需要更新自身 currentTerm，并且转换为 Follower 状态。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;若 &lt;code&gt;reply.Success&lt;/code&gt; 会表明达成一致，更新 &lt;code&gt;rf.matchIndex[server]&lt;/code&gt; 和 &lt;code&gt;rf.nextIndex[server]&lt;/code&gt;；否则仅更新 &lt;code&gt;rf.nextIndex[server]&lt;/code&gt;（这里涉及到 3 中的优化）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rf *Raft) doAppendEntries(server int) {
    //! make a lock for every follower to serialize
    select {
    case &amp;lt;- rf.chanAppend[server]:
    default: return
    }
    defer func() {rf.chanAppend[server] &amp;lt;- true}()

    for rf.getRole() == Leader {
        rf.mu.Lock()
        var args AppendEntriesArgs
        ...             // set variable value
        if rf.nextIndex[server] &amp;lt; len(rf.logTable) {
            args.Entry = rf.logTable[rf.nextIndex[server]:]
        }
        rf.mu.Unlock()

        var reply AppendEntriesReply
        if rf.sendAppendEntries(server, args, &amp;amp;reply) {
            if rf.role != Leader {
                return
            }
            if args.Term != rf.currentTerm || reply.Term &amp;gt; args.Term {
                if reply.Term &amp;gt; args.Term {
                    ... // update term and role
                }
                return
            }
            if reply.Success {
                rf.matchIndex[server] = reply.NextIndex - 1
                rf.nextIndex[server] = reply.NextIndex
            } else {
                rf.nextIndex[server] = reply.NextIndex
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;AppendEntries RPC handler&lt;/strong&gt; (2016.8.5 Update)&lt;/p&gt;

&lt;p&gt;一个机器接收到含有 log entry 的 AppendEntries RPC，前一部分跟处理 heartbeat 一样，剩下的部分则是决定是否更新自身的 log。根据 Raft paper 中的 figure 2 按部就班实现就好了。&lt;/p&gt;

&lt;p&gt;Update：处理 log entries array 还是需要注意一下：首先决定是否更新 log table，当满足条件决定更新后，主要存在三种更新情况：（1）更新旧 index；（2）更新旧 index，且添加新 index；（3）仅添加新 index。（如下图）&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://7vij5d.com1.z0.glb.clouddn.com/raft-appendentries.bmp&#34; alt=&#34;raft-appendentries&#34; /&gt;&lt;/p&gt;

&lt;p&gt;本文方法是：首先记录下当前开始处理的 &lt;code&gt;basicIndex := args.PrevLogIndex + 1&lt;/code&gt;（即 &lt;code&gt;args.Entry&lt;/code&gt; 中的起始 index），然后遍历 &lt;code&gt;args.Entry&lt;/code&gt;，一旦 index 已达 logTable 末端或者某个 entry 的 term 不匹配，则清除 logTable 中当前及后续的 entry，并且将 &lt;code&gt;args.Entry&lt;/code&gt; 的当前及后续 entry 添加到 logTable中。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;AppendEntries RPC optimization&lt;/strong&gt; (2016.8.5 Update)&lt;/p&gt;

&lt;p&gt;这一部分主要是优化 AppendEntries RPC，当发生拒绝 append 时减少 master 需要发送 RPC 的次数。虽然 Raft paper 中怀疑这个优化操作是否有必要，但 lab2 中有些 unreliable test case 就需要你实现这个优化操作…&lt;/p&gt;

&lt;p&gt;首先需要在 struct AppendEntriesReply 中添加两个数据：&lt;code&gt;NextIndex int&lt;/code&gt;,  &lt;code&gt;FailTerm int&lt;/code&gt;，分别用于指示 conflicting entry 所在 term，以及在该 term 中存储的第一个 entry 的 index；&lt;/p&gt;

&lt;p&gt;在 handler 的优化如下：如果 log unmatched，存在两种情况：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;len(rf.logTable) &amp;lt;= args.PrevLogIndex&lt;/code&gt;：则直接返回 &lt;code&gt;NextIndex = len(rf.logTable)&lt;/code&gt; 即可；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rf.logTable[args.PrevLogIndex].Term != args.PrevLogTerm&lt;/code&gt;：首先记录 &lt;code&gt;FailTerm&lt;/code&gt;，然后从当前 conflicting entry 开始向前扫描，直到 index 为 0 或者 term 不匹配，然后记录下该 term 的第一个 index；
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在 sender 的优化如下：当 append 失败时，如果 &lt;code&gt;NextIndex &amp;gt; matchIndex[server]&lt;/code&gt;，则 &lt;code&gt;nextIndex[server]&lt;/code&gt; 直接退至 &lt;code&gt;NextIndex&lt;/code&gt;，减少了需要尝试 append 的 RPC 次数；否则 &lt;code&gt;nextIndex[server]&lt;/code&gt; 退回到 &lt;code&gt;matchIndex[server] + 1&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STEP 4. Apply committed entries to local service replica&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;对于 Leader 而言，需要不断检测当前 term 的 log entry 的 replica 操作是否完成，然后进行 commit 操作。&lt;/p&gt;

&lt;p&gt;当超过半数的机器已经完成 replica 操作，则 Leader 认为该条 log entry 可以 commit。&lt;/p&gt;

&lt;p&gt;一旦当前 term 的某条 log entry L 是通过上述方式 commit 的，则根据 Raft 的 &lt;strong&gt;Log Matching Property&lt;/strong&gt;，Leader 可以 commit 先于 L 添加到 log 的所有 entry。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (rf *Raft) updateLeaderCommit() {
    rf.mu.Lock()
    defer rf.mu.Unlock()
    defer rf.persist()
    oldIndex := rf.commitIndex
    newIndex := oldIndex
    for i := len(rf.logTable)-1; i&amp;gt;oldIndex &amp;amp;&amp;amp; rf.logTable[i].Term==rf.getCurrentTerm(); i-- {
        countServer := 1
        for server := range rf.peers {
            if server != rf.me &amp;amp;&amp;amp; rf.matchIndex[server] &amp;gt;= i {
                countServer++
            }
        }
        if countServer &amp;gt; len(rf.peers) / 2 {
            newIndex = i
            break
        }
    }
    if oldIndex == newIndex {
        return
    }
    rf.commitIndex = newIndex

    //! update the log added in previous term
    for i := oldIndex + 1; i &amp;lt;= newIndex; i++ {
        rf.chanCommitted &amp;lt;- ApplyMsg{Index:i, Command:rf.logTable[i].Command}
        rf.lastApplied = i
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于 Follower 而言，则：&lt;code&gt;commitIndex = min(leaderCommit, index of last new entry)&lt;/code&gt;。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;persistence&#34;&gt;Persistence&lt;/h2&gt;

&lt;p&gt;最后一步是持久化保存 persistent state，不过在 lab2 仅是通过 &lt;code&gt;persister&lt;/code&gt; object 来保存，并没有真正使用到磁盘。实现了这一部分后可以稳定地 pass 掉关于 Persist 的 test case。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Read persist&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在 Make Raft peer 时读取 persister。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Write persist&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;修改了 Raft 的 persistent state 后应当及时写至 persister，主要是在以下几个地方插入 write：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动了 election 修改了自身的 persistent state后；&lt;/li&gt;
&lt;li&gt;受到了 RequestVote RPC 和 AppendEntries RPC 的 reply，得知需要更新自身的 persistent state 后；&lt;/li&gt;
&lt;li&gt;RequestVote RPC handler 和 AppendEntries RPC handler 处理完毕后；&lt;/li&gt;
&lt;li&gt;Leader 收到 client 的请求命令，添加到自身的 log 后。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;defect&#34;&gt;Defect&lt;/h2&gt;

&lt;p&gt;本文还有些不足，有待后续优化。主要是&lt;strong&gt;不能稳定地&lt;/strong&gt; pass &lt;code&gt;TestUnreliableAgree()&lt;/code&gt; + &lt;strong&gt;不能&lt;/strong&gt; pass &lt;code&gt;TestFigure8Unreliable()&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;To Be Continue&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;2016-8-11-update&#34;&gt;2016-8-11 Update&lt;/h2&gt;

&lt;p&gt;最近几天 debug 了之前的代码，发现了若干个问题。目前的代码实现已经&lt;strong&gt;比较稳定&lt;/strong&gt;地 pass 所有的 test case，上面的代码段也已经修改了。但仍然存在一个 bug: 在过 TestUnreliableAgree() 时 fail to reach agreement。这个 bug 的触发几率很低，还没想明白哪里出问题。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;LEADER ELECTION: Role Transfer (state machine)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;之前实现 server state machine 的方法是利用 channel 来进行状态修改操作，并且会新开一个 goroutine 进行该状态死循环，而没有其他状态的处理逻辑。本文由一个 goroutine 阻塞读取该 channel，从而处理 server 的状态转换。然而，channel 同步是存在延时的，只有在当前 goroutine 被挂起或者休眠等时，才会转去处理。&lt;/p&gt;

&lt;p&gt;这样的方法存在一个 bug：新 Leader 产生后，旧 Leader 收到消息应该更新自身的 term 并且转换为 Follower；然而由于 channel 同步并非立即执行，旧 Leader 在自身状态被重新赋值前仍然会执行 Leader 的代码；这时候就会出现两个 Leader 同时处理 RPC。&lt;/p&gt;

&lt;p&gt;因此，修改的内容是：去掉 channel 同步方法，当需要进行状态转换时，立即修改 server 的状态，终止该 server 当前状态的执行。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;LOG REPLICATION: AppendEntries RPC&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;之前不能 pass &lt;code&gt;TestUnreliableAgree()&lt;/code&gt; 和 &lt;code&gt;TestFigure8Unreliable()&lt;/code&gt; 时丝毫没有提示，只能等程序运行时间过长然后被杀死。后来思考了一下问题原因：&lt;strong&gt;程序运行过慢，跟不上 test case 要求的速度，所以后续的测试代码也根本没有执行&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;然后发现程序执行慢的原因是：Leader 发送 AppendEntries RPC 每次仅携带一条 log entry，导致 server 无法快速 catch up。所以修改的主要内容就是：AppendEntries RPC 每次携带从 &lt;code&gt;nextIndex&lt;/code&gt; 直到最新的 log entries。&lt;/p&gt;

&lt;h2 id=&#34;reference&#34;&gt;Reference&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.douban.com/note/549229678/&#34;&gt;MIT 6.824 Week 3 notes&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>微软2016校招在线笔试题解</title>
      <link>http://wiesen.github.io/post/%E5%BE%AE%E8%BD%AF2016%E6%A0%A1%E6%8B%9B%E5%9C%A8%E7%BA%BF%E7%AC%94%E8%AF%95%E9%A2%98%E8%A7%A3/</link>
      <pubDate>Fri, 15 Apr 2016 21:31:33 +0800</pubDate>
      
      <guid>http://wiesen.github.io/post/%E5%BE%AE%E8%BD%AF2016%E6%A0%A1%E6%8B%9B%E5%9C%A8%E7%BA%BF%E7%AC%94%E8%AF%95%E9%A2%98%E8%A7%A3/</guid>
      <description>

&lt;p&gt;微软2016校招在线笔试题解
题目地址：&lt;a href=&#34;http://hihocoder.com/contest/mstest2016april1/problems&#34;&gt;mstest2016april1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;感受：一定要理解好题意，注意细节。另外微软特别喜欢考察反向思维。&lt;/p&gt;

&lt;h2 id=&#34;a-font-size&#34;&gt;A. Font Size&lt;/h2&gt;

&lt;p&gt;选择一个最大字号刚好可以令页数不超过给定阈值P。&lt;/p&gt;

&lt;p&gt;逻辑理清即可。页由行组成，行由字符组成，其中字符为方形。需要注意的地方是：(1) 每段都从新的一行开始；(2) 每页至少显示一个字符。&lt;/p&gt;

&lt;p&gt;字号是整数，可以暴力求解遍历直到合适字号，单点时间复杂度为&lt;code&gt;O(10^6)&lt;/code&gt;，没有超过题目限制。但如果题目再卡紧点就有点危险了，所以更好的方法是利用二分查找，注意好边界条件即可。&lt;/p&gt;

&lt;p&gt;编码思路如下：&lt;/p&gt;

&lt;p&gt;1.确定字号最小值为1，最大值为 &lt;code&gt;min(W, H)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;2.求字号为中值 &lt;code&gt;m&lt;/code&gt; 时的页数：每段所占行数 &lt;code&gt;a[i]/(W/m)&lt;/code&gt;，对所有段所占行数求和，最后求所占页数 &lt;code&gt;total/(H/m)&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;3.当所占页数大于等于阈值P，则&lt;code&gt;r=m&lt;/code&gt;，小于则&lt;code&gt;l=m+1&lt;/code&gt;，直到&lt;code&gt;l&amp;gt;=r&lt;/code&gt;。注意等于阈值P时并不代表已经找到解，字号可能还能增大。&lt;/p&gt;

&lt;p&gt;4.解为 &lt;code&gt;r-1&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&#34;b-403-forbidden&#34;&gt;B. 403 Forbidden&lt;/h2&gt;

&lt;p&gt;题目的要求是进行 IP 地址匹配，返回最先（序号最小）匹配到的规则的动作，没有匹配的规则则返回 allow。&lt;/p&gt;

&lt;p&gt;一种方法是利用前缀树求解。建树方法: 在插入新规则 new 时，在该规则的前缀路径上（含等长）已有规则 old，意味着 old 屏蔽了new，直接丢弃新规则new。
由此，在匹配一个 IP 地址时，只需要返回前缀树上匹配这个 IP 地址的最长规则。&lt;/p&gt;

&lt;p&gt;建树时间复杂度为 &lt;code&gt;O(N)&lt;/code&gt;，匹配一次只需要常数时间，整体时间复杂度为 &lt;code&gt;O(min{N,M})&lt;/code&gt;。&lt;/p&gt;

&lt;h2 id=&#34;c-demo-day&#34;&gt;C. Demo Day&lt;/h2&gt;

&lt;p&gt;动态规划水题，起始位置为 (1,1)，动作为向下或向右，考虑好各个状态转移,并且注意边界特殊情况即可。时间复杂度为 &lt;code&gt;O(N*M)&lt;/code&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;DP[i][j][k]&lt;/code&gt; 含义为：在第 i 行 第 j 列时，向 k 方向前进需要改变多少个格子 &lt;code&gt;(1&amp;lt;=i&amp;lt;=N, 1&amp;lt;=j&amp;lt;=M, k=right or down)&lt;/code&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DP[i][j][right] = min{DP[i][j-1][right], DP[i-1][j][down] + (i + 1 &amp;lt; n &amp;amp;&amp;amp; maze[i+1][j] != &#39;b&#39;)} + (maze[i][j] == &#39;b&#39;);
DP[i][j][down] = min{DP[i-1][j][down], DP[i][j-1][right] + (j + 1 &amp;lt; m &amp;amp;&amp;amp; maze[i][j+1] != &#39;b&#39;)} + (maze[i][j] == &#39;b&#39;);
DP[i-1][j][k] 仅在 i - 1 &amp;gt; 0 时存在，同理 DP[i][j-1][k] 仅在 j - 1 &amp;gt; 0 时存在。
最后结果：ans = min{DP[N][M][right], DP[N][M][down])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;d-building-in-sandbox&#34;&gt;D. Building in Sandbox&lt;/h2&gt;

&lt;p&gt;思路参考&lt;a href=&#34;https://www.zhihu.com/question/42406890/answer/94388263&#34;&gt;知乎&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;题目的初始条件是地面([1,1,0]~[100,100,0])已经摆满了方块。&lt;/p&gt;

&lt;p&gt;对于新添加的方块有下面两个要求：
1. 相邻性：必须与之前之前放置的立方体(包括地面)相邻；
2. 可达性：必须能够从空间外部在不穿过任何方块的前提下到达该方块，换言之就是不能处于封闭空间中。&lt;/p&gt;

&lt;p&gt;对于第一个要求每次添加时进行简单的逻辑判断即可。&lt;/p&gt;

&lt;p&gt;第二个要求根据一位答主的想法，由闭空间判断转换为开空间判断，就变成了图论里的简单题目了：先对外部空间做一个FloodFill；然后倒序判断方块 &lt;code&gt;i&lt;/code&gt; 是否与外部空间相邻；相邻的话就删除该方块，然后做FloodFill；一旦存在不相邻就是处于封闭空间中。&lt;/p&gt;

&lt;p&gt;不过该方法是离线判断，强制在线可参考另一个&lt;a href=&#34;https://www.zhihu.com/question/42406890/answer/94480532&#34;&gt;回答&lt;/a&gt;。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;阅读参考&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zhihu.com/question/42406890&#34;&gt;https://www.zhihu.com/question/42406890&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.gotit.sinaapp.com/&#34;&gt;http://www.gotit.sinaapp.com/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://yfwz100.github.io/articles/interv/microsoft-2016.html&#34;&gt;http://yfwz100.github.io/articles/interv/microsoft-2016.html&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Flat Datacenter Storage Paper Review</title>
      <link>http://wiesen.github.io/post/Flat-Datacenter-Storage-Paper-Review/</link>
      <pubDate>Wed, 30 Mar 2016 21:33:37 +0800</pubDate>
      
      <guid>http://wiesen.github.io/post/Flat-Datacenter-Storage-Paper-Review/</guid>
      <description>

&lt;p&gt;A review for paper Nightingale E B, Elson J, Fan J, et al. Flat datacenter storage[C]//Presented as part of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 12). 2012: 1-15.&lt;/p&gt;

&lt;h1 id=&#34;introduction&#34;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;What is FDS?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Flat Datacenter Storage (FDS) is a high-performance, fault-tolerant, large-scale, &lt;strong&gt;locality-oblivious&lt;/strong&gt; blob store.&lt;/li&gt;
&lt;li&gt;Using a novel combination of &lt;strong&gt;full bisection bandwidth networks&lt;/strong&gt;, &lt;strong&gt;data and metadata striping&lt;/strong&gt;, and** flow control**, FDS multiplexes an application’s large-scale I/O across the available throughput and latency budget of every disk in a cluster.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;How is the Performance?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2 GByte/sec per client.&lt;/li&gt;
&lt;li&gt;Recover from lost disk (92 GB) in 6.2 seconds.&lt;/li&gt;
&lt;li&gt;It sets the world-record for disk-to-disk sorting in 2012 for MinuteSort: 1,033 disks and 256 computers (136 tract servers, 120 clients), 1,401 Gbyte in 59.4s.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;architecture&#34;&gt;Architecture&lt;/h1&gt;

&lt;p&gt;High-level design &amp;ndash; a common pattern&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://7vij5d.com1.z0.glb.clouddn.com/fds-flat.png&#34; alt=&#34;comparison&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Right distributed model - GFS &amp;amp; HDFS: Data is either on a local disk or a remote disk.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Move the computation to data. Location awareness adds &lt;strong&gt;complexity&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Why move? Because remote data access is slow.&lt;/li&gt;
&lt;li&gt;Why slow? Because the network is oversubscribed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Left distributed model &amp;ndash; FDS: Object storage assuming no oversubscription. Data is &lt;em&gt;all&lt;/em&gt; remote.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Separates the storage from computation.&lt;/li&gt;
&lt;li&gt;No local vs. remote disk distinction&lt;/li&gt;
&lt;li&gt;simpler work schedulers&lt;/li&gt;
&lt;li&gt;simpler programming models&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://7vij5d.com1.z0.glb.clouddn.com/fds-architectures.png&#34; alt=&#34;architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Above it&amp;rsquo;s the architecture of FDS:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Lots of clients, and lots of storage servers (&amp;ldquo;tractservers&amp;rdquo;)&lt;/li&gt;
&lt;li&gt;Partition the data, and master (&amp;ldquo;metadata server&amp;rdquo;) controls partitioning&lt;/li&gt;
&lt;li&gt;Replica groups for reliability&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;deisign-overview&#34;&gt;Deisign Overview&lt;/h1&gt;

&lt;h2 id=&#34;how-to-store-data-blobs-and-tracts&#34;&gt;How to store data? &amp;ndash; Blobs and Tracts&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Data is logically stored in blobs.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A blob is a byte sequence named with a 128-bit GUID.&lt;/li&gt;
&lt;li&gt;Blobs can be &lt;em&gt;any length&lt;/em&gt; up to the system’s storage capacity.&lt;/li&gt;
&lt;li&gt;Blobs are divided into &lt;strong&gt;tracts&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Tracts are the units responsible for read and write&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tracts are &lt;em&gt;sized&lt;/em&gt; such that random and sequential access achieves nearly the same throughput.&lt;/li&gt;
&lt;li&gt;The tract size is set when the cluster is created based upon cluster hardware.(64kb~8MB)&lt;/li&gt;
&lt;li&gt;All tracts’ metadata is &lt;strong&gt;cached in memory&lt;/strong&gt;, eliminating many disk accesses.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Every disk is managed by a process called a tractserver:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Services read and write requests from clients.&lt;/li&gt;
&lt;li&gt;Lay out tracts &lt;strong&gt;directly to disk by using the raw disk interface&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Provides API, and the API features are follow:

&lt;ul&gt;
&lt;li&gt;Tract reads are not guaranteed to arrive in order of issue. Writes are not guaranteed to be committed in   order of issue.&lt;/li&gt;
&lt;li&gt;Tractserver writes are &lt;em&gt;atomic&lt;/em&gt;: a write is either committed or failed completely.&lt;/li&gt;
&lt;li&gt;Calls are &lt;em&gt;asynchronous&lt;/em&gt;: using callback, allows deep pipelining to achieve good performance.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Weak&lt;/em&gt; consistency to clients&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;how-to-organize-and-manage-metadata-deterministic-data-placement&#34;&gt;How to organize and manage metadata? &amp;ndash; Deterministic data placement&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Many systems solve this problem using a metadata server that stores the location of data blocks.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Advantage: allowing maximum flexibility of data placement and visibility into the system’s state.&lt;/li&gt;
&lt;li&gt;Drawbacks: the metadata server is a central point of failure, usually implemented as a replicated state machine.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;FDS uses a metadata server, but it&amp;rsquo;s role is simple and limited:  &lt;strong&gt;tract locator table&lt;/strong&gt; (&lt;strong&gt;TLT&lt;/strong&gt;):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;collect a list of the system’s active tractservers and distribute it to clients.&lt;/li&gt;
&lt;li&gt;With &lt;em&gt;k-way&lt;/em&gt; replication, each entry has the address of &lt;em&gt;k&lt;/em&gt; tractservers.&lt;/li&gt;
&lt;li&gt;Weighted by disk speed&lt;/li&gt;
&lt;li&gt;Only update when cluster changes&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Compute a tract index to read or write, which is designed to both be deterministic and produce uniform disk utilization: &lt;em&gt;Tract_Locator&lt;/em&gt; = TLT[(Hash(&lt;em&gt;GUID&lt;/em&gt;) + &lt;em&gt;Tract&lt;/em&gt;) % len(TLT)]&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Hash(&lt;em&gt;GUID&lt;/em&gt;)&lt;/strong&gt;: Randomize blob&amp;rsquo;s tractserver, even if GUIDs aren&amp;rsquo;t random (uses SHA-1)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Tract&lt;/em&gt;&lt;/strong&gt;: adds tract number outside the hash, so large blobs use all TLT entries uniformly&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Compute a tract index for Blob metadata, which enable to distribute Blob metadata: &lt;em&gt;Tract_Locator&lt;/em&gt; = TLT[(Hash(&lt;em&gt;GUID&lt;/em&gt;) - 1) % len(TLT)]&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The metadata server isn&amp;rsquo;t a single point failure.&lt;/li&gt;
&lt;li&gt;Parallelized operation can be servied in parallel by independent tractservers.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;To summarize, FDS metadata scheme has following properties:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The metadata server is in the critical path &lt;em&gt;only&lt;/em&gt; when a client process starts.&lt;/li&gt;
&lt;li&gt;The TLT can be &lt;em&gt;cached long-term&lt;/em&gt;, eliminating all traffic to the metadata server under normal conditions.&lt;/li&gt;
&lt;li&gt;TLT contains random permutations of the list of tractservers, which make sequential reads and writes parallel.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;what-kind-of-application-will-will-not-benefic-from-fds-dynamic-work-allocation&#34;&gt;What kind of application will /will not benefic from FDS? &amp;ndash; Dynamic Work Allocation&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Since &lt;strong&gt;storage and compute are no longer colocated&lt;/strong&gt;, the assignment of work to worker can be done &lt;em&gt;dynamically&lt;/em&gt; at fine granularity &lt;em&gt;during&lt;/em&gt; task execution, which enables FDS to &lt;strong&gt;mitigate stragglers&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;The &lt;strong&gt;best practice&lt;/strong&gt; for FDS applications is to centrally (or, at large scale, hierarchically) give small units of work to each worker as it nears completion of its previous unit.&lt;/li&gt;
&lt;li&gt;Such a scheme is &lt;strong&gt;not practical&lt;/strong&gt; in systems where the assignment of work to workers is fixed in advance by the requirement that data be resident at a particular worker before the job begins.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;replication-and-failure-recovery&#34;&gt;Replication and Failure Recovery&lt;/h1&gt;

&lt;h2 id=&#34;replication&#34;&gt;Replication&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Serialized Blob operation &lt;em&gt;Create&lt;/em&gt;, &lt;em&gt;Delete&lt;/em&gt;, &lt;em&gt;Extend&lt;/em&gt;: client writes to primary, primary executes a &lt;strong&gt;two-phase commit&lt;/strong&gt; with replicas.&lt;/li&gt;
&lt;li&gt;Write to &lt;em&gt;all&lt;/em&gt; replicas, read from &lt;em&gt;random&lt;/em&gt; replica&lt;/li&gt;
&lt;li&gt;Supports per-blob &lt;strong&gt;variable replication&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;failure-recovery&#34;&gt;Failure recovery&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;each ertry in TLT has a &lt;strong&gt;version number&lt;/strong&gt; to control update, and &lt;em&gt;all&lt;/em&gt; operations as well.&lt;/li&gt;
&lt;li&gt;Transient failures: &lt;strong&gt;partial failure recovery&lt;/strong&gt; that complete failure recovery or use other replicas to recover the writes that the returning tractserver missed.&lt;/li&gt;
&lt;li&gt;Cascading tractserver failures: fill more slots in the TLT&lt;/li&gt;
&lt;li&gt;Concurrent tractserver failures: detected as missing TLT entries, and execute normal failure recovery protocol.&lt;/li&gt;
&lt;li&gt;Metadata server failures: using Paxos leader election&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;replicated-data-layout&#34;&gt;Replicated data layout&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;The selection of which k disks appear has an important impact on both durability and recovery speed&lt;/li&gt;
&lt;li&gt;A better TLT has &lt;em&gt;O(n^2)&lt;/em&gt; entries, so each possible pair of disks appears in anentry of the TLT.

&lt;ul&gt;
&lt;li&gt;First, performance during recovery involves &lt;em&gt;every disk&lt;/em&gt; in the cluster.&lt;/li&gt;
&lt;li&gt;a triple disk failure within the recovery window has only about a &lt;em&gt;2/n&lt;/em&gt; chance of causing permanent data loss.&lt;/li&gt;
&lt;li&gt;adding more replicas decreases the probability of data loss.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cluster-growth&#34;&gt;Cluster growth&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Rebalances the assignment of TLT entries so that both existing data and new workloads are uniformly distributed.&lt;/li&gt;
&lt;li&gt;These assignments happen in two phases (&lt;code&gt;pending&lt;/code&gt; and &lt;code&gt;commits&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;If a new tractserver fails while its TLT entries are pending, increments the TLT entry version and expunges it.&lt;/li&gt;
&lt;li&gt;new tractservers must read from the existing tractserver with a superset of the data required.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;networking&#34;&gt;Networking&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;Network bandwidth = disk bandwidth&lt;/li&gt;
&lt;li&gt;Full bisection bandwidth is stochastic&lt;/li&gt;
&lt;li&gt;Short flows good for ECMP&lt;/li&gt;
&lt;li&gt;TCP &lt;em&gt;hates&lt;/em&gt; short flows, but RTS/CTS to mitigate incast&lt;/li&gt;
&lt;li&gt;FDS works &lt;em&gt;great&lt;/em&gt; for Blob Storate on CLOS networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Reference：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://www.binospace.com/index.php/flat-datacenter-storage-system-analysis/&#34;&gt;Flat DataCenter Storage之系统分析&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://cs.stackexchange.com/questions/23163/how-does-fds-flat-datacenter-storage-make-optimizations-around-locality-unnece&#34;&gt;How does FDS (flat datacenter storage) make optimizations around locality unnecessary?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=YbOjxCxtMpU&#34;&gt;Youtube video&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Chain Replication Paper Review</title>
      <link>http://wiesen.github.io/post/Chain-Replication-Paper-Review/</link>
      <pubDate>Sat, 19 Mar 2016 21:33:09 +0800</pubDate>
      
      <guid>http://wiesen.github.io/post/Chain-Replication-Paper-Review/</guid>
      <description>

&lt;p&gt;本文是读完 Van Renesse R, Schneider F B. Chain Replication for Supporting High Throughput and Availability[C]//OSDI. 2004. 的总结。&lt;/p&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;Chain replication is a new approach to coordinating clusters of fail-stop storage servers.&lt;/p&gt;

&lt;p&gt;Chain replication 采用 ROWAA (read one, write all available) 方法, 具有良好的 Scalability.&lt;/p&gt;

&lt;p&gt;该方法目的是, &lt;strong&gt;不以牺牲强一致性为代价来实现高吞吐和高可用&lt;/strong&gt;, 从而提供分布式存储服务.&lt;/p&gt;

&lt;h2 id=&#34;a-storage-service-interface&#34;&gt;A Storage Service Interface&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Clients&lt;/strong&gt; 发送 query 或 update 操作 request&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The storage service&lt;/strong&gt; 为每个 request 生成 reply 发送会 client 告知其已经接收或已经处理完成, 从而 client 可以得知某 request 是否接收成功以及是否处理完成.&lt;/p&gt;

&lt;p&gt;Client request type:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;query(objId, opts) -&amp;gt; value&lt;/code&gt;: retrieve current value of &lt;em&gt;opts&lt;/em&gt; of &lt;em&gt;objId&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;update(objId, newVal, opts) -&amp;gt; value&lt;/code&gt;: update &lt;em&gt;opts&lt;/em&gt; of &lt;em&gt;objId&lt;/em&gt; with &lt;em&gt;newVal&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Client&amp;rsquo;s view of an object:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;State is:
    Hist[objId]: history of all updates to objId
    Pending[objId]: set of pending requests for objId
Transitions are:
    T1: Client request r arrives: 
        Pending[objId] += {r}
    // 一个 client request 接收失败 = server 忽略了该 client request
    T2: Client request r ∈ Pending[objId] ignored: 
        Pending[objId] -= {r}
    T3: Client request r ∈ Pending[objId] processed: 
        Pending[objId] -= {r}
        if r = query(objId, opts) then 
            reply according options opts based on Hist[objId]
        else if r = update(objId, newVal, opts) then
            Hist[objId] := Hist[objId] · r
            reply according options opts based on Hist[objId]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;chain-replication-protocol&#34;&gt;Chain Replication Protocol&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1. Assumptions: 所有服务器均假设为 fail-stop&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;each server halts in response to a failure&lt;/li&gt;
&lt;li&gt;a server’s halted state can be detected by the environment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Protocol Details&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;在 chain replication 中, 所有 servers 根据 &lt;em&gt;objID&lt;/em&gt; 线性排列从而组成一个链表.
&lt;img src=&#34;http://7vij5d.com1.z0.glb.clouddn.com/the%20chain.png&#34; width=&#34;400&#34;/&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;所有 update 操作由 HEAD 结点接收并开始处理, 然后按照FIFO顺序向链表中的下一个节点传递, 直到该 update 操作被 TAIL 节点处理.&lt;/li&gt;
&lt;li&gt;所有 query 操作由 TAIL 结点接收并处理.&lt;/li&gt;
&lt;li&gt;所有 query 操作 / update 操作的确认由 TAIL 结点处理 (即发送 reply 给 client).
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Coping with Server Failures&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;论文中构建一个 &lt;em&gt;master&lt;/em&gt; server, 其主要功能如下 (为区分本文将其余负责数据存储的 server 称为 &lt;em&gt;data&lt;/em&gt; server):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;检测其余 &lt;em&gt;data&lt;/em&gt; servers 的失败&lt;/li&gt;
&lt;li&gt;在链表新增或删除节点时, 通知 &lt;em&gt;data&lt;/em&gt; servers 更新 predecessor 及 successor&lt;/li&gt;
&lt;li&gt;告知 client 链表的 HEAD 节点和 TAIL 节点&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;论文中假设 &lt;em&gt;master&lt;/em&gt; server 永不崩溃。而实际上该论文的 prototype 利用 Paxos 协调 &lt;em&gt;master&lt;/em&gt; server 的各个 replicas 从而 behave in aggregate like a single process that does not fail, 以此避免单点故障.&lt;/p&gt;

&lt;p&gt;下面仅讨论 &lt;em&gt;data&lt;/em&gt; server 故障, 即如何在链表中的节点出现故障时保证存储服务的强一致性.主要分为以下头节点, 尾节点和中间节点三种情况.&lt;/p&gt;

&lt;p&gt;论文中阐述了两个性质:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Update Propagation Invariant (更新传递不变性)&lt;/strong&gt;. 意即对于编号 &lt;em&gt;i&lt;/em&gt; 和 &lt;em&gt;j&lt;/em&gt;, 若有: &lt;img src=&#34;http://latex.codecogs.com/gif.latex?i%20%5Cle%20j&#34; alt=&#34;fomula&#34; /&gt; (例如 &lt;em&gt;i&lt;/em&gt; 是 &lt;em&gt;j&lt;/em&gt; 的 predecessor), 则有:  successor 的 update 操作序列 是 predecessor 的前缀 —— &lt;img src=&#34;http://latex.codecogs.com/gif.latex?Hist%5Ej_%7BobjID%7D%20%5Cpreceq%20Hist%5Ei_%7BobjID%7D&#34; alt=&#34;fomula&#34; /&gt; (该性质根据链表节点间 update 操作由 FIFO 传递得出)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inprocess Requests Invariant (上下文请求不变性)&lt;/strong&gt;. 每个 server &lt;em&gt;i&lt;/em&gt; 维护一个列表 &lt;img src=&#34;http://www.forkosh.com/mathtex.cgi?Sent_i}&#34;&gt;, 其中存储着 server &lt;em&gt;i&lt;/em&gt; 已经处理并传递给 successor 节点但可能未被 tail 节点处理的 update requests. 当 tail 节点处理了一个 update request &lt;em&gt;r&lt;/em&gt; 后会发送确认 &lt;em&gt;ack&amp;reg;&lt;/em&gt; 给 predecessor, server &lt;em&gt;i&lt;/em&gt; 接收 &lt;em&gt;ack&amp;reg;&lt;/em&gt; 后将 &lt;em&gt;r&lt;/em&gt; 从 &lt;img src=&#34;http://www.forkosh.com/mathtex.cgi?Sent_i}&#34;&gt; 中删除, 然后依次向前传递. 据此, 若有: &lt;img src=&#34;http://latex.codecogs.com/gif.latex?Hist%7B%5Ei_%7BobjID%7D%7D%20%3D%20Hist%7B%5Ej_%7BobjID%7D%7D%20%5Coplus%20Sent_i&#34; alt=&#34;fomula&#34; /&gt; (根据 tail 节点接收到的 request 必定已被其所有 predecessors 接收到这一事实得出)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Head 节点故障停止&lt;/strong&gt;: 将 Head 节点从链表中移除, 其 successor 节点称为新的 Head 节点. 旧 Head 节点已经传递的 update 操作继续传递, 而丢失的 update 操作可视为 server 忽略了该 update (如前所述等同于server 接收该 client request 失败), 因此对应的 client request 将无法接收到 reply, 此时 client 会 resend request. 不影响存储服务的强一致性.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tail 节点故障停止&lt;/strong&gt;: 将 Tail 节点从链表中移除, 其前继 Tail- 节点称为新的 Tail 节点. 由于 &lt;img src=&#34;http://latex.codecogs.com/gif.latex?Tail%5E-%20%3C%20Tail%20%5Cto%20Hist%7B%5E%7Btail%7D_%7BobjID%7D%7D%20%5Cpreceq%20Hist%7B%5E%7BTail%5E-%7D_%7BobjID%7D%7D&#34; alt=&#34;fomula&#34; /&gt;, 从用户的角度看即数据变新变多了, 因此并不影响存储服务的读一致性.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;中间节点故障停止&lt;/strong&gt;: 将故障节点 S 从链表中移除, 然后 &lt;em&gt;master&lt;/em&gt; server 首先通知故障节点的后继 S+ 节点新的链表配置, 然后通知前继 S- 节点连接后继 S+ 节点并要求其处理 &lt;img src=&#34;http://www.forkosh.com/mathtex.cgi?Sent^-}&#34;&gt; 中的 update request, 后序 update 操作继续传递下去, 因此也不影响存储服务的强一致性.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;扩展链表&lt;/strong&gt;: 当越来越多故障节点被移除, 链表将会缩短, 同时容错性下降.因此当链表变短时应当向链表中添加新的 servers. 理论上可以在链表的任何位置添加新 server, 但最简单的方法是在结尾添加 T+ 节点. 首先 T+ 通过复制 &lt;img src=&#34;http://www.forkosh.com/mathtex.cgi? Hist{^T_{objID}}&#34;&gt; 到 &lt;img src=&#34;http://www.forkosh.com/mathtex.cgi? Hist{^{T^+}_{objID}}&#34;&gt; 完成初始化, 然后 T+ 节点一边处理由 T 节点 forward 过来的 query request 一边处理 &lt;img src=&#34;http://www.forkosh.com/mathtex.cgi?Sent^T}&#34;&gt;, 最后 T+ 正式作为新的 tail 节点.&lt;/p&gt;

&lt;h2 id=&#34;comparision-to-primary-backup-protocols&#34;&gt;Comparision to Primary/Backup Protocols&lt;/h2&gt;

&lt;p&gt;Chain replication 是 primary/backup 方法的一种改进, 实际上是一种副本管理的状态机方法.&lt;/p&gt;

&lt;p&gt;在 primary/backup 方法中, 有一个 &lt;em&gt;primary&lt;/em&gt; server, 负责序列化 client requests (从而保证强一致性), 然后将序列化的 client requests 或 resulting updates 分散发送到各个 &lt;em&gt;backup&lt;/em&gt; servers, 等待接收非故障 &lt;em&gt;backups&lt;/em&gt; 的确认信息, 最后发送 reply 给 client. 当 &lt;em&gt;primary&lt;/em&gt; server 故障停止, 其中一个 &lt;em&gt;backup&lt;/em&gt; server 将提升为 &lt;em&gt;primary&lt;/em&gt; server.&lt;/p&gt;

&lt;p&gt;相比 primary/backup 方法, Chain replication 的不同如下:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;client requests 由两个 replicas 处理, 即 head 节点负责序列化处理 update request, tail 节点并行处理 query requests, 降低了 query requests 的 latency.&lt;/li&gt;
&lt;li&gt;Chain replication 只能串行传递 update requests, 因此发送 reply 的 latency 与 the sum of server latencies 成比例.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;当出现 server 故障停止时, Chain replication 和 primary/backup 方法的主要时延都是检测 server failure, 其次是 recovery.&lt;/p&gt;

&lt;h2 id=&#34;simulation-experiments&#34;&gt;Simulation Experiments&lt;/h2&gt;

&lt;p&gt;论文在四种情况下进行实验:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Single Chain, No Failures&lt;/li&gt;
&lt;li&gt;Multiple Chains, No Failures&lt;/li&gt;
&lt;li&gt;Effects of Failures on Throughput&lt;/li&gt;
&lt;li&gt;Large Scale Replication of Critical Data&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;additional-application&#34;&gt;Additional: Application&lt;/h2&gt;

&lt;p&gt;在 &lt;a href=&#34;https://www.usenix.org/legacy/events/usenix09/tech/full_papers/terrace/terrace.pdf&#34;&gt;CRAQ&lt;/a&gt; 论文中介绍了基于 chain replication 的 CRAQ 系统, 该系统扩展了 chain replication protocol, 使链表上的所有节点均可处理 query 操作, 提高系统的吞吐, 同时仍然提供强一致性保证.&lt;/p&gt;

&lt;p&gt;微软云计算平台 &lt;a href=&#34;http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/11-calder.pdf&#34;&gt;Windows Azure&lt;/a&gt;、&lt;a href=&#34;http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/11-calder.pdf&#34;&gt;FDS&lt;/a&gt; 都使用 chain replication protocol 提供强一致性保证.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Reference&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nEbD-qutsKo&#34;&gt;https://www.youtube.com/watch?v=nEbD-qutsKo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/yfkiss/article/details/13772669&#34;&gt;http://blog.csdn.net/yfkiss/article/details/13772669&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://blog.xiaoheshang.info/?p=883&#34;&gt;http://blog.xiaoheshang.info/?p=883&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>快速排序笔记</title>
      <link>http://wiesen.github.io/post/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Tue, 15 Mar 2016 21:31:20 +0800</pubDate>
      
      <guid>http://wiesen.github.io/post/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E7%AC%94%E8%AE%B0/</guid>
      <description>

&lt;p&gt;快排最引人注目的特点是原地排序（只需要一个很小的辅助栈），且将长度为 &lt;em&gt;N&lt;/em&gt; 的数组排序所需的时间和 &lt;em&gt;NlgN&lt;/em&gt; 成正比。
而快排的主要缺点是非常脆弱，在实现时必须非常小心才能避免性能低下。&lt;/p&gt;

&lt;p&gt;同前述一致：以 C++ 实现,仅针对 vector 进行操作, 并且遵循 C++ 左闭右开的区间标准: [a,b)。&lt;/p&gt;

&lt;h2 id=&#34;基本算法&#34;&gt;基本算法&lt;/h2&gt;

&lt;p&gt;快排也是一种分治的排序算法，快排与归并排序是互补的：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;归并排序将数组分成两个子数组分别排序，再将有序子数组归并以将整个数组排序，其递归调用发生在处理整个数组&lt;strong&gt;之前&lt;/strong&gt;；&lt;/li&gt;
&lt;li&gt;而快排则是先确定 &lt;code&gt;key&lt;/code&gt; 的位置，而后当两个子数组都有序时则整个数组即有序，其递归调用发生在处理整个数组&lt;strong&gt;之后&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;快排的关键在划分，划分过程需要使得数组满足以下三个条件：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;对于某个 &lt;code&gt;key&lt;/code&gt;，&lt;code&gt;a[key]&lt;/code&gt; 已经排定；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[low,key)&lt;/code&gt; 中所有元素都不大于 &lt;code&gt;a[key]&lt;/code&gt;；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[key,high)&lt;/code&gt; 中所有元素都不小于 &lt;code&gt;a[key]&lt;/code&gt;。
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://7vij5d.com1.z0.glb.clouddn.com/quick.png&#34; alt=&#34;quick&#34; /&gt;&lt;/p&gt;

&lt;p&gt;据此，快排的基本思想是:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;选取一个元素作为切分元素 &lt;code&gt;key&lt;/code&gt;，然后从数组左端开始向右扫描直到找到一个大于等于它的元素，再从数组右端开始向左扫描直到找到一个小于等于它的元素，而后交换它们的位置;&lt;/li&gt;
&lt;li&gt;如此继续，就可以保证左指针 &lt;code&gt;left&lt;/code&gt; 的左侧元素都不大于 &lt;code&gt;key&lt;/code&gt;，右指针 &lt;code&gt;right&lt;/code&gt; 的右侧都不小于 &lt;code&gt;key&lt;/code&gt;;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;当左右指针相遇时 &lt;code&gt;（left &amp;gt;= right）&lt;/code&gt;，将切分元素 key 与左子数组最右元素（也就是 &lt;code&gt;right&lt;/code&gt; 最终停下的位置）交换然后返回划分位置 &lt;code&gt;right&lt;/code&gt; 即可。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;int Partition(std::vector&amp;lt;T&amp;gt; &amp;amp;vec, int low, int high) {
    int left = low, right = high;
    while (true) {
        // Avoid subscript beging out of range
        while (vec[++left] &amp;lt; vec[low]) if (left == high - 1)break;
        while (vec[low] &amp;lt; vec[--right]) if (right == low)break;
        // Terminate the loop
        if (left &amp;gt;= right)break;
        std::swap(vec[left], vec[right]);
    }
    std::swap(vec[low], vec[right]);
    return right;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;快排中有若干细节问题值得注意，否则会导致实现错误或者性能下降:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;越界：这里遵循左闭右开原则 [a,b)，因此左侧不能访问最右元素（右侧可以访问最左元素），否则将会出现越界。本文在内循环中左指针的 break 处理上与右指针有所差异：&lt;code&gt;if (left == high - 1) break&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;随机性：快排是一个&lt;strong&gt;随机化&lt;/strong&gt;的算法，因此在排序前需将数组随机打乱，在 C++ 中使用    &lt;code&gt;std::random_shuffle&lt;/code&gt; 函数对数组进行处理。将数组随机化的原因是希望能够预测并依赖该算法的性能特点，后续本文阐述。&lt;/li&gt;
&lt;li&gt;循环：当左指针大于或等于右指针时终于划分&lt;code&gt;Partition()&lt;/code&gt;中的循环,最后将切分元素 &lt;code&gt;key&lt;/code&gt; 放入正确位置： &lt;code&gt;std::swap(vec[key], vec[right])&lt;/code&gt;。而事实上， &lt;code&gt;left&lt;/code&gt; 和 &lt;code&gt;right&lt;/code&gt; 的位置正好一致或相邻。当一侧元素全部小于或大于 &lt;code&gt;key&lt;/code&gt; 时，则 &lt;code&gt;left == right&lt;/code&gt;，否则 &lt;code&gt;left - 1 == right&lt;/code&gt;。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;递归：当子数组为空或所含元素仅为1个时则无需再处理，在左闭右开区间 [a,b) 中即为 &lt;code&gt;b - a &amp;lt;= 1&lt;/code&gt;。否则继续处理，同时为了贯彻左闭右开原则，代码中的递归如下:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void NormalQuick(std::vector&amp;lt;T&amp;gt; &amp;amp;vec, int low, int high) {
    // Terminate the recursive
    if (high - low &amp;lt;= 1){
        Insertion(vec, low, high);
        return;
    }
    int nCut = Partition(vec, low, high);
    // Detail: Use range[low, high)
    NormalQuick(vec, low, nCut);
    NormalQuick(vec, nCut + 1, high);
｝
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;算法改进&#34;&gt;算法改进&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;切换到插入排序&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;和大多数递归排序算法一样，改进快排性能的一个简单办法是在排序小树组时切换到插入排序，从而避免递归调用，并且对于小数组来说快排比插入排序慢。&lt;/p&gt;

&lt;p&gt;经验表明，在大多数情况下 &lt;code&gt;CUTOFF&lt;/code&gt; 取值 5~15 能够取得比较好的性能。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if (high - low &amp;lt;= CUTOFF) {
    Insertion(vec, low, high);
    return;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;三取样切分&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;待补充&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;熵最优快排——三向切分&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;所谓的“熵最优”是指：对于任意分布的输入，最优的基于比较的排序算法平均所需的比较次数与三向切分快排平均所需的比较次数相比，处于常数因子范围之内。当然前提是需要将数组进行随机化。&lt;/p&gt;

&lt;p&gt;进一步使用信息论来解释快排性能可参考 Algs4 中的快排章节，Mackayd的&lt;a href=&#34;http://users.aims.ac.za/~mackay/sorting/sorting.html&#34;&gt;Heapsort, Quicksort, and Entropy&lt;/a&gt;。刘未鹏的&lt;a href=&#34;http://mindhacks.cn/2008/06/13/why-is-quicksort-so-quick/&#34;&gt;数学之美番外篇：快排为什么那样快&lt;/a&gt;也值得一看，浅显易懂。&lt;/p&gt;

&lt;p&gt;三向切分快排的运行时间和输入的信息量的 &lt;em&gt;N&lt;/em&gt; 倍成正比。对于含有大量重复元素的数组，它将快排的排序时间从&lt;strong&gt;线性对数级&lt;/strong&gt;降低到&lt;strong&gt;线性级别&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;基本想法是将数组切分为三部分，分别对应小于、等于和大于切分元素的数组元素。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dijkstra 三向切分&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Dijkstra 提出的“三向切分快速排序”极为简洁。这里的额外交换用于和切分元素&lt;strong&gt;不等&lt;/strong&gt;的元素。 其从左到右遍历数组一次：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;维护指针 &lt;code&gt;left&lt;/code&gt; 使得 &lt;code&gt;[low, left)&lt;/code&gt; 中的元素均小于 key；&lt;/li&gt;
&lt;li&gt;维护一个指针 &lt;code&gt;right&lt;/code&gt; 使得 &lt;code&gt;[right, high)&lt;/code&gt; 中的元素均大于 key；&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nMid&lt;/code&gt; 使得 &lt;code&gt;[left, nMid)&lt;/code&gt; 中的元素均等于 key；&lt;/li&gt;
&lt;li&gt;余下 &lt;code&gt;[nMid, right)&lt;/code&gt; 中的元素尚未排定。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://7vij5d.com1.z0.glb.clouddn.com/3-wayquick.png&#34; alt=&#34;3wayquick&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void OptimalQuick(std::vector&amp;lt;T&amp;gt; &amp;amp;vec, int low, int high) {
    if (high - low &amp;lt;= CUTOFF){
        Insertion(vec, low, high);
        return;
    }
    int left = low, nMid = low + 1, right = high - 1;
    T key = vec[low];
    while (nMid &amp;lt;= right)
    {
        if (vec[nMid] &amp;lt; key){
            std::swap(vec[left], vec[nMid]);
            left++; nMid++;
        }
        else if (key &amp;lt; vec[nMid]){
            std::swap(vec[nMid], vec[right]);
            right--;
        }
        else nMid++;
    }
    OptimalQuick(vec, low, nMid);
    OptimalQuick(vec, right + 1, high);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;快速三向切分（J.Bently, D.McIlroy)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;通过将重复元素置于子数组两端，从而实现一个信息量最优的排序算法。该方法与上述方法是等价的，只是快速三向切分中的额外交换用于和切分元素&lt;strong&gt;相等&lt;/strong&gt;的元素。过程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;维护指针 &lt;code&gt;p&lt;/code&gt; 和 &lt;code&gt;q&lt;/code&gt; 使得 &lt;code&gt;vec[low, p)&lt;/code&gt; 和 &lt;code&gt;vec[q, high)&lt;/code&gt; 中的元素都和 &lt;code&gt;key&lt;/code&gt; 相等;&lt;/li&gt;
&lt;li&gt;维护指针 &lt;code&gt;i&lt;/code&gt; 和 &lt;code&gt;j&lt;/code&gt; 使得 &lt;code&gt;vec[p, i)&lt;/code&gt; 中的元素小于 &lt;code&gt;key&lt;/code&gt; ，&lt;code&gt;vec[j, q)&lt;/code&gt; 中的元素大于 &lt;code&gt;key&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;在切分循环结束后将和 &lt;code&gt;key&lt;/code&gt; 相等的元素交换到正确的位置上。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://7vij5d.com1.z0.glb.clouddn.com/bmquick.png&#34; alt=&#34;quickX&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;void QuickX(std::vector&amp;lt;T&amp;gt; &amp;amp;vec, int low, int high) {
    if (high - low &amp;lt;= CUTOFF){
        Insertion(vec, low, high);
        return;
    }

    int p = low, q = high, i = low, j = high;
    T key = vec[low];
    while (true){
        while (vec[++i] &amp;lt; key)if (i == high - 1)break;
        while (key &amp;lt; vec[--j])if (j == low)break;
        if (i == j &amp;amp;&amp;amp; key == vec[i]){
            std::swap(vec[++p], vec[i]);
        }
        if (i &amp;gt;= j)break;
        std::swap(vec[i], vec[j]);
        // exchange only when equal to key
        if(vec[i] == key)std::swap(vec[++p], vec[i]);
        if(vec[j] == key)std::swap(vec[--q], vec[j]);
    }
    // exchange to right position
    i = j + 1;
    while (p &amp;gt;= low)std::swap(vec[p--], vec[j--]);
    while (q &amp;lt; high)std::swap(vec[q++], vec[i++]);
    // recursive
    QuickX(vec, low, j + 1);
    QuickX(vec, i, high);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;p&gt;Reference：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;http://algs4.cs.princeton.edu/home/&#34;&gt;Algs4&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://users.aims.ac.za/~mackay/sorting/sorting.html&#34;&gt;Heapsort, Quicksort, and Entropy&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://mindhacks.cn/2008/06/13/why-is-quicksort-so-quick/&#34;&gt;数学之美番外篇：快排为什么那样快&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>归并排序笔记</title>
      <link>http://wiesen.github.io/post/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Thu, 10 Mar 2016 21:24:46 +0800</pubDate>
      
      <guid>http://wiesen.github.io/post/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%E7%AC%94%E8%AE%B0/</guid>
      <description>

&lt;p&gt;归并排序是一种渐近最优的基于比较排序的算法, 意即: 其在最坏情况下的比较次数和任意基于比较的排序算法所需的最少比较次数都是~NlgN。其主要的优点是可以保证将任意长度为 N 的数组排序的时间复杂度为 O(NlgN); 其主要缺点是空间复杂度为 O(N)。&lt;/p&gt;

&lt;p&gt;这里均以 C++ 实现, 为了避免陷入到语言细节里所以仅针对 vector 进行操作, 并且遵循 C++ &lt;strong&gt;左闭右开&lt;/strong&gt;的区间标准: [a,b)。值得注意的是区间表示一致性这个细节对针对数组和搜索(如二分查找)的算法有着举足轻重的影响。&lt;/p&gt;

&lt;p&gt;至于倾向于 [a,b) 左闭右开区间表示的原因可参考&lt;a href=&#34;http://stackoverflow.com/questions/9963401/why-are-standard-iterator-ranges-begin-end-instead-of-begin-end&#34;&gt;链接&lt;/a&gt;。简单来说主要原因有二: 一是 end-begin 即可得到区间大小; 二是当区间退化为 0 时包含左界更加&amp;rdquo;natural&amp;rdquo;。&lt;/p&gt;

&lt;h1 id=&#34;自顶向下的归并排序&#34;&gt;自顶向下的归并排序&lt;/h1&gt;

&lt;p&gt;递归实现的归并排序是算法设计中&lt;strong&gt;分治思想&lt;/strong&gt;的典型应用。&lt;/p&gt;

&lt;p&gt;主要是两个函数: 由 &lt;code&gt;MergeSort()&lt;/code&gt; 负责递归调用, &lt;code&gt;Merge()&lt;/code&gt; 负责归并。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;优化一: 对小规模子数组使用插入排序&lt;/p&gt;

&lt;p&gt;递归会使小规模问题中方法的调度过于频繁, 而插入或者选择在小数组上比归并要快, 所以改进对小规模子数组的处理方法可以改进整个算法。根据经验, 使用插入处理小规模子数组(&amp;lt;15)可将归并的运行时间缩短10%~15%。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;优化二: 测试子数组是否有序&lt;/p&gt;

&lt;p&gt;添加一个判断条件: &lt;code&gt;if (a[mid] &amp;gt; a[mid+1])&lt;/code&gt; 再进行 &lt;code&gt;Merge()&lt;/code&gt; 操作, 否则数组已经是有序的。进行此优化可以令任意有序的子数组算法时间复杂度变为线性。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;优化三: 不将元素复制到辅助数组&lt;/p&gt;

&lt;p&gt;在递归调用的每个层次交换输入数组和辅助数组的角色, 可节省将数组元素复制到用于归并的辅助数组的时间(无法节省空间)。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;template&amp;lt;class T&amp;gt;
void Merge(std::vector&amp;lt;T&amp;gt; &amp;amp;src, std::vector&amp;lt;T&amp;gt; &amp;amp;dest, int nHead, int nMid, int nEnd) {
    int nLeftIndex = nHead, nRightIndex = nMid;
    for (int i = nHead; i &amp;lt; nEnd; ++i){
        // Detail: Use range[nHead, nEnd)
        if (nLeftIndex &amp;gt;= nMid) dest[i] = src[nRightIndex++];
        else if (nRightIndex &amp;gt;= nEnd) dest[i] = src[nLeftIndex++];
        else if (src[nLeftIndex] &amp;lt; src[nRightIndex]) dest[i] = src[nLeftIndex++];
        else dest[i] = src[nRightIndex++];
    }
}

template&amp;lt;class T&amp;gt;
void MergeSort(std::vector&amp;lt;T&amp;gt; &amp;amp;src, std::vector&amp;lt;T&amp;gt; &amp;amp;dest, int nHead, int nEnd) {
    // if (nEnd - nHead &amp;lt;= 1) return; // Before optimizing
    // Optimization 1: Use InsertSort when small scale
    if (nEnd - nHead &amp;lt;= 15) {
        InsertSort(dest, nHead, nEnd);
        return;
    }
    int nMid = (nHead + nEnd) / 2;
    // Optimization 2: Avoid copying to auxiliary array
    MergeSort(dest, src, nHead, nMid);
    MergeSort(dest, src, nMid, nEnd);
    // Optimization 3: If the sub-array is sorted then skip merge
    if (src[nMid - 1] &amp;lt;= src[nMid]){
        std::copy(src.begin() + nHead, src.begin() + nEnd, dest.begin() + nHead);
    }
    else Merge(src, dest, nHead, nMid, nEnd);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;自底向上的归并排序&#34;&gt;自底向上的归并排序&lt;/h1&gt;

&lt;p&gt;实现归并排序的另一种方法是化递归为循环自底向上进行归并, 即先归并微型子数组, 然后再成对归并得到的子数组, 如此这般直至将整个数组归并在一起。该实现比递归方法代码量少, 但复杂度一样。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;template&amp;lt;class T&amp;gt;
void Merge(std::vector&amp;lt;T&amp;gt; &amp;amp;vec, int nHead, int nMid, int nEnd) {
    std::vector&amp;lt;T&amp;gt; tmp(vec);
    int nLeftIndex = nHead, nRightIndex = nMid;
    for (int i = nHead; i &amp;lt; nEnd; ++i){
        if (nLeftIndex &amp;gt;= nMid) vec[i] = tmp[nRightIndex++];
        else if (nRightIndex &amp;gt;= nEnd) vec[i] = tmp[nLeftIndex++];
        else if (tmp[nLeftIndex] &amp;lt; tmp[nRightIndex]) vec[i] = tmp[nLeftIndex++];
        else vec[i] = tmp[nRightIndex++];
    }
}

template&amp;lt;class T&amp;gt;
void MergeSortBU(std::vector&amp;lt;T&amp;gt; &amp;amp;vec, int nHead, int nEnd) {
    int nLength = nEnd - nHead;
    for (int sz = 1; sz &amp;lt; nLength; sz += sz){
        for (int i = nHead; i &amp;lt; nLength - sz; i = i + sz + sz){
            int nMin = nLength &amp;lt; i + sz + sz ? nLength : i + sz + sz;
            Merge(vec, i, i + sz, nMin);
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Algorithms, 4th Edition&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/acgtyrant/Algorithm-and-Data-Structure/wiki&#34;&gt;https://github.com/acgtyrant/Algorithm-and-Data-Structure/wiki&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://stackoverflow.com/questions/9963401/why-are-standard-iterator-ranges-begin-end-instead-of-begin-end&#34;&gt;http://stackoverflow.com/questions/9963401/why-are-standard-iterator-ranges-begin-end-instead-of-begin-end&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>