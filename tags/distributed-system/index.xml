<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distributed System on Wiesen&#39;s Blog</title>
    <link>https://wiesen.github.io/tags/distributed-system/</link>
    <description>Recent content in Distributed System on Wiesen&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Sep 2016 17:32:46 +0800</lastBuildDate>
    
	<atom:link href="https://wiesen.github.io/tags/distributed-system/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>MIT 6.824: Lab 4 Sharded KeyValue Service Implementation</title>
      <link>https://wiesen.github.io/post/mit-6.824-lab4-sharded-keyvalue-service/</link>
      <pubDate>Fri, 09 Sep 2016 17:32:46 +0800</pubDate>
      
      <guid>https://wiesen.github.io/post/mit-6.824-lab4-sharded-keyvalue-service/</guid>
      <description>lab4 是基于 lab2 和 lab3 实现的 Raft Consensus Algorithm 之上实现 Sharded KeyValue Service。主要分为两部分：
 Part A：The Shard Master Part B: Sharded Key/Value Server  除了最后一个 challenge test case TestDelete() 以外，目前代码其余都可以 pass。但偶尔会 fail 在 unreliable test case，目前的定位是 raft 的实现还有点 bug。
Code Link:
 PART A PART B  Architecture lab4 的架构是典型的 M/S 架构（a configuration service and a set of replica groups)，不过实现十分基础，很多功能没有实现：1) shards 之间的传递很慢并且不允许 concurrent client acess；2) 每个 raft group 中的 member 不会改变。</description>
    </item>
    
    <item>
      <title>MIT 6.824: lab3 Fault-Tolerant Key/Value Service Implementation</title>
      <link>https://wiesen.github.io/post/mit-6.824-lab3-fault-tolerant-kvservice-implementation/</link>
      <pubDate>Sat, 06 Aug 2016 17:32:46 +0800</pubDate>
      
      <guid>https://wiesen.github.io/post/mit-6.824-lab3-fault-tolerant-kvservice-implementation/</guid>
      <description>lab3 是基于 lab2 实现的 Raft Consensus Algorithm 之上实现 KV Service。主要分为两部分：
 Part A：Key/value service without log compaction，即实现基本的分布式存储服务。 Part B: Key/value service with log compaction，即在 Part A 基础上实现 log compaction。  代码分别可以 pass 各个 test case，但所有一起跑时有时会卡在 TestPersistPartition() 这里，初步猜测是 raft 的实现还有点 bug。
Code Link
Part A：Key/value service  KV Database Client API
key/value database 的 client API 必须满足以下要求：
 保证仅执行一次(at most once semantics)：API 必须为每个 Client 及 每个 Request 赋予唯一的 id； 必须向使用该 API 的应用提供 sequential consistency：对于每个 Client，仅有一条 Request RPC 在显式执行(利用 lock 实现)；  此外，API 应当一直尝试向 key/value server 发起 RPC 直到收到 positive reply；并且记住 leader id，从而尽可能避免失败次数。</description>
    </item>
    
    <item>
      <title>MIT 6.824: lab2 Raft Consensus Algorithm Implementation</title>
      <link>https://wiesen.github.io/post/mit-6.824-lab2-raft-consensus-algorithm-implementation/</link>
      <pubDate>Fri, 10 Jun 2016 21:32:46 +0800</pubDate>
      
      <guid>https://wiesen.github.io/post/mit-6.824-lab2-raft-consensus-algorithm-implementation/</guid>
      <description>Raft 将一致性问题分为了三个相对独立的子问题，分别是：
 Leader election：当前 leader 崩溃时，集群中必须选举出一个新的 leader； Log replication：leader 必须接受来自 clients 的 log entries，并且将其 replicate 到集群机器中，强制其余 logs 与其保持一致； Safety：Raft 中最关键的 safety property 是 State Machine Safety Property，亦即是，当任一机器 apply 了某一特定 log entry 到其 state machine 中，则其余服务器都不可能 apply 了一个 index 相同但 command 不同的 log。  差不多依据上述划分，6.824 中 Raft 的实现指导逻辑还是挺清晰的，其中 safety property 由 Leader election 和 Log replication 共同承担，并且将 Persistence 作为最后一部分。实现过程主要分为：
 Leader Election and Heartbeats：首先令 Raft 能够在不存在故障的情景下选举出一个 leader，并且稳定保持状态； Log Replication：其次令 Raft 能够保持一个 consistent 并且 replicated 的 log； Persistence：最后令 Raft 能够持久化保存 persistent state，这样在重启后可以进行恢复。  其中，本文主要参考 Raft paper，其中的 figure 2 作用很大。本文实现大量依赖 channel 实现消息传递和线程同步。</description>
    </item>
    
    <item>
      <title>Flat Datacenter Storage Paper Review</title>
      <link>https://wiesen.github.io/post/flat-datacenter-storage-paper-review/</link>
      <pubDate>Wed, 30 Mar 2016 21:33:37 +0800</pubDate>
      
      <guid>https://wiesen.github.io/post/flat-datacenter-storage-paper-review/</guid>
      <description>A review for paper Nightingale E B, Elson J, Fan J, et al. Flat datacenter storage[C]//Presented as part of the 10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 12). 2012: 1-15.
Introduction What is FDS?
 Flat Datacenter Storage (FDS) is a high-performance, fault-tolerant, large-scale, locality-oblivious blob store. Using a novel combination of full bisection bandwidth networks, data and metadata striping, and** flow control**, FDS multiplexes an application’s large-scale I/O across the available throughput and latency budget of every disk in a cluster.</description>
    </item>
    
    <item>
      <title>Chain Replication Paper Review</title>
      <link>https://wiesen.github.io/post/chain-replication-paper-review/</link>
      <pubDate>Sat, 19 Mar 2016 21:33:09 +0800</pubDate>
      
      <guid>https://wiesen.github.io/post/chain-replication-paper-review/</guid>
      <description>本文是读完 Van Renesse R, Schneider F B. Chain Replication for Supporting High Throughput and Availability[C]//OSDI. 2004. 的总结。
Summary Chain replication is a new approach to coordinating clusters of fail-stop storage servers.
Chain replication 采用 ROWAA (read one, write all available) 方法, 具有良好的 Scalability.
该方法目的是, 不以牺牲强一致性为代价来实现高吞吐和高可用, 从而提供分布式存储服务.
A Storage Service Interface Clients 发送 query 或 update 操作 request
The storage service 为每个 request 生成 reply 发送会 client 告知其已经接收或已经处理完成, 从而 client 可以得知某 request 是否接收成功以及是否处理完成.</description>
    </item>
    
  </channel>
</rss>